{
    "id": "a9daaf82592cef33fc6c079112ef6d18b158533e",
    "title": "Coming to its senses: Lessons learned from Approximating Retro\ufb01tted BERT representations for Word Sense information",
    "abstract": "Retro\ufb01tting static vector space word repre-001 sentations using external knowledge bases 002 has yielded substantial improvements in their 003 lexical-semantic capacities but is non-trivial to 004 apply to contextual word embeddings (CWE). 005 In this paper, we propose M AKE S ENSE , a 006 method that \u2018approximates\u2019 retro\ufb01tting in 007 CWEs to better infer word sense knowledge 008 from word contexts. We speci\ufb01cally analyze 009 BERT and M AKE S ENSE -transformed BERT 010 representations over a diverse set of exper-011 iments encompassing sense-sensitive similar-012 ities, alignment with human-elicited similar-013 ity judgments, and probing tasks focusing on 014 sense distinctions and hypernymy. Our \ufb01nd-015 ings indicate that M AKE S ENSE imparts sub-016 stantial improvements in word sense informa-017 tion over vanilla CWEs but largely preserves 018 more complex usage of sense and directionally 019 sensitive information such as hypernymy. 020"
}