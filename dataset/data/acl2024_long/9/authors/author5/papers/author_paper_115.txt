{
    "id": "cf5dd2c28f5eec168a8545218b8e7d35f5e0210d",
    "title": "Autoregressive Language Model for Zero-shot Constrained Keyphrase Generation",
    "abstract": "Recently, most of the state-of-the-art 001 keyphrase prediction models are based on 002 a supervised generative model. It shows 003 signi\ufb01cantly better than before. Nevertheless, 004 it still faces domain robustness and building 005 datasets on high-resource. To overcome these 006 limitations, unsupervised methods have also 007 been critical and studied. We analyzed it also 008 have a defect in a necessary process, which 009 extracts candidates beforehand selecting 010 keyphrase. As not including various forms of 011 phrases, we note that the unsupervised method 012 can\u2019t ensure oracle keyphrase. In this paper, 013 we present zero-shot constrained keyphrase 014 generation by leveraging a large-scale lan-015 guage model. To generate diverse keyphrases, 016 we explore controlling a phrase during the 017 generation. Finally, we evaluate benchmark 018 datasets of the scholar domain. It results in 019 better performances than unsupervised meth-020 ods on several datasets without going through 021 the candidate extraction stage. For domain 022 robustness, we evaluate out-of-domain DUC 023 compare with NUS. Since our method doesn\u2019t 024 \ufb01ne-tune to a corpus of a speci\ufb01c domain, 025 it\u2019s better than supervised methods based on 026 Sequence-to-Sequence. 027"
}