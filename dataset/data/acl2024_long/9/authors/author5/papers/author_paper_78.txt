{
    "id": "201a73830bc82848f212e00adf852dc4f243953a",
    "title": "Crossword: Estimating Unknown Embeddings using Cross Attention and Alignment Strategies",
    "abstract": "Word embedding methods like word2vec and 001 GloVe have been shown to learn strong rep-002 resentations of words. However, these meth-003 ods only learn representations for words in 004 the training corpus. This is problematic, as 005 models using these representations need ways 006 to handle unknown and new words, known 007 as out-of-vocabulary (OOV) words. As a re-008 sult, there have been multiple attempts to learn 009 OOV word representations in a similar fash-010 ion to how humans learn new words, using 011 surrounding words (\u201ccontext clues\") and word 012 roots/subwords. However, most current ap-013 proaches suffer from two problems. First, 014 these models calculate context clue estimates 015 and subword estimates separately and then 016 combine them shallowly for a \ufb01nal estimate, 017 therefore ignoring potentially important infor-018 mation each type can learn from the other. 019 Secondly, although subword embeddings are 020 trained to estimate word vectors, we \ufb01nd these 021 embeddings don\u2019t occupy the same space as 022 word embeddings. Current models do not take 023 this into account, and do not align the spaces 024 before combining them. In response to this, 025 we propose Crossword , a transformer based 026 OOV estimation model that combines context 027 and subwords at the attention level, allowing 028 each type to in\ufb02uence the other for a stronger 029 \ufb01nal estimate. Crossword successfully com-030 bines these different sources of information 031 using cross attention, along with strategies to 032 align subword and context spaces. 033"
}