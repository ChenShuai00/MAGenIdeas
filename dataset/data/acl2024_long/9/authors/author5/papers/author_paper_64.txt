{
    "id": "eae05ca173d4d5cdfab7c691dd4ea96afe97d6e5",
    "title": "CQM robust : A Chinese Dataset of Linguistically Perturbed Natural Questions for Evaluating the Robustness of Question Matching Models",
    "abstract": "In this paper, we focus on studying robustness 001 evaluation of Chinese question matching. Most 002 of the previous work on analyzing robustness 003 issue focus on just one or a few types of arti-004 ficial adversarial examples. Instead, we argue 005 that it is necessary to formulate a comprehen-006 sive evaluation about the linguistic capabilities 007 of models on natural texts. For this purpose, 008 we create a Chinese dataset namely CQM robust 009 which contains natural questions with linguis-010 tic perturbations to evaluate the robustness of 011 question matching models. CQM robust contains 012 3 categories and 13 subcategories with 32 lin-013 guistic perturbations. The extensive experi-014 ments demonstrate that CQM robust has a better 015 ability to distinguish different models. Impor-016 tantly, the detailed breakdown of evaluation by 017 linguistic phenomenon in CQM robust helps us 018 easily diagnose the strength and weakness of 019 different models. Additionally, our experiment 020 results show that the effect of artificial adversar-021 ial examples does not work on the natural texts. 022 The dataset and baseline codes will be publicly 023 available in the open source community. 024"
}