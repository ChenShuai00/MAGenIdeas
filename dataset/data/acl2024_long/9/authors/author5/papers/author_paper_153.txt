{
    "id": "16c9cbb532c7c6f73ba35cfcf37992ed5aabcfe6",
    "title": "Towards Efficient Federated Multilingual Modeling with LoRA-based Language Family Clustering",
    "abstract": "Federated Multilingual Modeling (FMM) plays 001 a crucial role in the applications of natural lan-002 guage processing due to the increasing diver-003 sity of languages and the growing demand for 004 data privacy. However, FMM faces limitations 005 stemming from the substantial communication 006 costs in networking and the conflicts arising 007 from parameter interference between different 008 languages. To address these challenges, we 009 introduce a communication-efficient federated 010 learning framework with low-rank adaptation 011 and language family clustering for Multilingual 012 Modeling (MM). In this framework, we main-013 tain the weights of the base model, exclusively 014 updating the lightweight Low-rank adaptation 015 (LoRA) parameters to minimize communica-016 tion costs. Additionally, we mitigate parameter 017 conflicts by grouping languages based on their 018 language family affiliations, as opposed to ag-019 gregating all LoRA parameters. Experiments 020 demonstrate that our proposed model not only 021 surpasses the baseline models in performance 022 but also reduces the communication overhead. 023"
}