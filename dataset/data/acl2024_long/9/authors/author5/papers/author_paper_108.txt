{
    "id": "b88703f68f4abb9e69b86fb42dff85aa4a76fca4",
    "title": "Multilingual Translation from Denoising Pre-Training",
    "abstract": "Recent work demonstrates the potential of training one model for multilingual machine translation. In parallel, denoising pretraining using unlabeled monolingual data as a starting point for \ufb01netuning bitext machine translation systems has demonstrated strong performance gains. However, little has been explored on the potential to combine denoising pretraining with multilingual machine translation in a single model. In this work, we \ufb01ll this gap by studying how multilingual translation models can be created through multilingual \ufb01netuning . Fintuning multilingual model from a denoising pretrained model incorporates the bene\ufb01ts of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is rare. Further, we create the ML50 benchmark to facilitate re-producible research by standardizing training and evaluation data. On ML50, we show that multilingual \ufb01netuning signi\ufb01cantly improves over multilingual models trained from scratch and bilingual \ufb01netuning for translation into English. We also \ufb01nd that multilingual \ufb01ne-tuning can signi\ufb01cantly improve over multilingual models trained from scratch for zero-shot translation on non-English directions. Finally, we discuss that the pretraining and \ufb01netuning paradigm alone is not enough to address the challenges of multilingual models for to-Many directions performance."
}