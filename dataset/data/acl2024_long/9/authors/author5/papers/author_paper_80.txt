{
    "id": "2e996fdf0b6929f9ab44195d62e261afc4a18f69",
    "title": "Compressing Sentence Representation via Homomorphic Projective Distillation",
    "abstract": "How to learn highly compact yet effective sen-001 tence representation? Pre-trained language 002 models have been effective in many NLP tasks. 003 However, these models are often huge and pro-004 duce large sentence embeddings. Moreover, 005 there is a big performance gap between large 006 and small models. In this paper, we propose 007 H omomorphic P rojective D istillation (HPD) to 008 learn compressed sentence embeddings. Our 009 method augments a small Transformer encoder 010 model with learnable projection layers to pro-011 duce compact representations while mimicking 012 a large pre-trained language model to retain the 013 sentence representation quality. We evaluate 014 our method with different model sizes on both 015 semantic textual similarity (STS) and semantic 016 retrieval (SR) tasks. Experiments show that our 017 method achieves 2.7-4.5 points performance 018 gain on STS tasks compared with previous best 019 representations of the same size. In SR tasks, 020 our method improves retrieval speed (8.2 \u00d7 ) 021 and memory usage (8.0 \u00d7 ) compared with state-022 of-the-art large models. 023"
}