{
    "id": "6af30352dc418669d6d3709ae1b94188818f77f3",
    "title": "Dissecting Inaccuracies in Large Language Models: An Analysis on Reasoning-Error Causes on Large Language Models",
    "abstract": "While large language models (LLMs) have 001 rapidly improved performance on a broad num-002 ber of tasks, they still lag behind in abstract 003 reasoning tasks. Wang et al. (2023) proposed 004 self-consistency , finding that sampling multi-005 ple rationales before taking a majority vote sta-006 bly improves performance in both mathemat-007 ical and commonsense reasoning. This work 008 augments self-consistency idea with a variety 009 of clustering and mapping approaches to bal-010 ance between diversity and accuracy, and ad-011 ditionally explore and evaluate sources of in-012 accuracies in reasoning performance more effi-013 ciently and concisely. We introduce two novel 014 techniques: identifying consensus responses by 015 clustering semantic embeddings of model out-016 puts, and systematically varying temperature 017 schedules during the course of generation. By 018 doing so, we aim to capture a more compre-019 hensive spectrum of reasoning paths employed 020 by the model and increase confidence in co-021 herent answers providing guidance about mod-022 els wrong doings while improving accuracy on 023 common benchmarks. 024"
}