{
    "id": "772e90809192a9ba1ea97d30a1ba0f0734fa4c65",
    "title": "Ef\ufb01cient Ensemble Transformer for Accurate Answer Sentence Ranking",
    "abstract": "Large transformer models can highly improve 001 Answer Sentence Selection (AS2) task, but 002 their high computational costs prevent their 003 use in many real world applications. In this 004 paper, we explore the following research ques-005 tion: How can we make the AS2 models 006 more accurate without signi\ufb01cantly increas-007 ing their model complexity? To address the 008 question, we propose a Multiple Heads Stu-009 dent architecture (MHS), an ef\ufb01cient neural 010 network designed to distill an ensemble of 011 large transformers into a single smaller model. 012 An MHS model consists of two components: 013 a stack of transformer layers that is used to en-014 code inputs, and a set of ranking heads; unlike 015 traditional distillation technique each of them 016 is trained by distilling a different large trans-017 former architecture in a way that preserves the 018 diversity of the ensemble members. The re-019 sulting model captures the knowledge of het-020 erogeneous transformer models by using just 021 a few extra parameters. We show the effec-022 tiveness of MHS on three English datasets for 023 AS2; our proposed approach outperforms all 024 single-model distillations we consider, rivaling 025 the state-of-the-art large AS2 models that have 026 2 . 7 \u00d7 more parameters and run 2 . 5 \u00d7 slower. 027"
}