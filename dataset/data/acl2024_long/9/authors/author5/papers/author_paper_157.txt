{
    "id": "662accae22d78e2ec8d7a4ed664e279bfe68abff",
    "title": "Sources of Hallucination by Large Language Models on Inference Tasks Anonymous EMNLP",
    "abstract": "Large Language Models (LLMs) are claimed 001 to be capable of Natural Language Inference 002 (NLI), necessary for applied tasks like question 003 answering and summarization. We present a 004 series of behavioral studies on several LLM 005 families (LLaMA, GPT-3.5, and Anonymous 006 LLM 1 ) which probe their behavior using con-007 trolled experiments. We establish two bi-008 ases originating from pretraining which predict 009 much of their behavior, and show that these 010 are major sources of hallucination in generative 011 LLMs. First, memorization at the level of sen-012 tences: we show that, regardless of the premise, 013 models falsely label NLI test samples as entail-014 ing when the hypothesis is attested in training 015 data, and that entities are used as \u201cindices\u201d to 016 access the memorized data. Second, statistical 017 patterns of usage learned at the level of cor-018 pora: we further show a similar effect when the 019 premise predicate is less frequent than that of 020 the hypothesis in the training data, a bias fol-021 lowing from previous studies. We demonstrate 022 that LLMs perform significantly worse on NLI 023 test samples which do not conform to these bi-024 ases than those which do, and we offer these as 025 valuable controls for future LLM evaluation. 2 026"
}