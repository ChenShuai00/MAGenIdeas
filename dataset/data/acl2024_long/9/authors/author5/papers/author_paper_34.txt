{
    "id": "3fad26446f542af6db07df290b000fabee355f25",
    "title": "Tree Knowledge Distillation for Compressing Transformer-Based Language Models",
    "abstract": "Knowledge distillation has emerged as a 001 promising technique for compressing neural 002 language models. However, most knowledge 003 distillation methods focus on extracting the 004 \u201cknowledge\u201d from a teacher network to guide 005 the training of a student network, ignoring the 006 \u201crequirements\u201d of the student. In this paper, 007 we introduce Tree Knowledge Distillation for 008 Transformer-based teacher and student mod-009 els, which allows student to actively extract 010 its \u201crequirements\u201d via a tree of tokens. In spe-011 ci\ufb01c, we \ufb01rst choose the [CLS] token at the 012 output layer of Transformer in student as the 013 root of the tree. We choose tokens with the 014 highest values in the row for [CLS] of the 015 attention feature map at the second last layer 016 as the children of the root. Then we choose 017 children of these nodes in their correspond-018 ing rows of the attention feature map at the 019 next layer, respectively. Later, we connect lay-020 ers of Transformer in student to correspond-021 ing layers in teacher by skipping every t lay-022 ers. At last, we improve the loss function by 023 adding the summed mean squared errors be-024 tween the embeddings of the tokens in the tree. 025 The experiments show that tree knowledge dis-026 tillation achieves competitive performance for 027 compressing BERT among other knowledge 028 distillation methods in GLUE benchmark. 029"
}