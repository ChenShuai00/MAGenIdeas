{
    "id": "e593419e03717fc4218f3c998dff0586c3e1d1fc",
    "title": "Improving Spoken Semantic Parsing using Unpaired Text from Textual Corpora and Large Language Model Prompting",
    "abstract": "Spoken semantic parsing (SSP) involves gen-001 erating machine-comprehensible parses from 002 input speech. Training robust models for ex-003 isting application domains represented in train-004 ing data or extending to new domains requires 005 corresponding triplets of speech-transcript-006 semantic parse data, which is expensive to ob-007 tain. In this paper, we address this challenge 008 by examining methods that can use or generate 009 transcript-semantic parse data (unpaired text) 010 without corresponding speech. First, when un-011 paired text is drawn from existing textual cor-012 pora, we compare Joint Audio Text (JAT) and 013 Text-to-Speech (TTS) as ways to use unpaired 014 text to generate speech representations. Experi-015 ments on the STOP dataset show that unpaired 016 text from existing and new domains improves 017 performance by 2% and 30% in absolute Exact 018 Match (EM) respectively. 019 Second, when unpaired text is not available 020 from existing textual corpora, Large Language 021 Models (LLMs) can be prompted to generate 022 unpaired text for existing and new domains, 023 and JAT or TTS can be used with the gener-024 ated unpaired text to improve SSP. Prior work 025 has mostly focused on using LLMs to generate 026 synthetic data for classification tasks. In this 027 paper, we introduce multiple prompting strate-028 gies to obtain synthetic data in existing and 029 new domains based on intent classes, intent-030 slot combinations and example transcripts and 031 parses. Experiments show that using synthetic 032 parse data with JAT for existing domains can 033 improve SSP performance on STOP by 1.4 % 034 absolute EM. Using synthetic parse data with 035 TTS for a new held-out domain improves EM 036 on STOP for the held out domain by 2.6% ab-037 solute."
}