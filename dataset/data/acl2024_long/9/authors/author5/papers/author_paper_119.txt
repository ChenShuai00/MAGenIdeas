{
    "id": "e2981b3c3b747b88ad8651b88080f415037cc1fc",
    "title": "Causal Transformers: Improving the Robustness on Spurious Correlations",
    "abstract": "The fully-connected dependencies in self-001 attention over-\ufb01t spurious correlations and 002 limit the generalization on out-of-distribution 003 data. Pre-trained language models (PLMs) al-004 leviate this problem bene\ufb01tted from the ap-005 preciable counterexamples in large-scale pre-006 training corpora. However, there is no study to 007 resolve this problem by improving the model 008 structure. We enforced the causal indepen-009 dence mechanism in the self-attention network, 010 which constrains attention mapping topologies 011 (AMGs) as causal structures. To implement 012 it, we de\ufb01ned a smooth loss on the Markov 013 boundary constrained directed acyclic graph 014 (DAG) with the Lagrange duality, and used it 015 to optimize the AMGs towards causal struc-016 tures. Further, this causal attention network 017 was applied on Transformer (Causal Trans-018 former). The empirical results on two spu-019 rious correlation challenging (SCC) datasets, 020 neural machine translation (NMT) and natural 021 language inference (NLI) tasks demonstrated 022 that our Causal Transformer outperforms the 023 state-of-the-art model and improves the out-of-024 distribution prediction. 025"
}