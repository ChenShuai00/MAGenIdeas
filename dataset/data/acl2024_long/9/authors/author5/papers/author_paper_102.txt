{
    "id": "a5705223fa0e8cd1adbca75b89ae9a571746081a",
    "title": "Hierarchical Transformer Networks for Long-sequence and Multiple Clinical Documents Classi\ufb01cation",
    "abstract": "We present a Hierarchical Transformer Net-001 work for modeling long-term dependencies 002 across clinical notes for the purpose of patient-003 level prediction. The network is equipped with 004 three levels of Transformer-based encoders to 005 learn progressively from words to sentences, 006 sentences to notes, and \ufb01nally notes to pa-007 tients. The \ufb01rst level from word to sentence 008 directly applies a pre-trained BERT model as 009 a fully trainable component. While the sec-010 ond and third levels both implement a stack 011 of transformer-based encoders, before the \ufb01nal 012 patient representation is fed into a classi\ufb01ca-013 tion layer for clinical predictions. Compared 014 to conventional BERT models, our model in-015 creases the maximum input length from 512 to-016 kens to much longer sequences that are appro-017 priate for modeling large numbers of clinical 018 notes. We empirically examine different hyper-019 parameters to identify an optimal trade-off 020 given computational resource limits. Our ex-021 periment results on the MIMIC-III dataset for 022 different prediction tasks demonstrate that the 023 proposed Hierarchical Transformer Network 024 outperforms previous state-of-the-art models, 025 including but not limited to B IG B IRD . 026"
}