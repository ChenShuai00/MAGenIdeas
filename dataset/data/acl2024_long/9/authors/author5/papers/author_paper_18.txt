{
    "id": "6ce31ab55c3cad599b91e1a36c5e2928d31e3986",
    "title": "What are Adapters Really Efficient At?",
    "abstract": "Adapters have been positioned as a parameter-001 efficient fine-tuning (PEFT) approach, whereby 002 a minimal number of parameters are added to 003 the model and fine-tuned. However, adapters 004 have not been sufficiently analyzed to under-005 stand if PEFT translates to benefits in train-006 ing/deployment efficiency and maintainabil-007 ity/extensibility. Through extensive experi-008 ments on many adapters, tasks, and languages 009 in supervised and cross-lingual zero-shot set-010 tings, we clearly show that for Natural Lan-011 guage Understanding tasks, the parameter ef-012 ficiency in adapters does not translate to effi-013 ciency gains compared to full fine-tuning of 014 models. More precisely, adapters are relatively 015 expensive to train and have slightly higher de-016 ployment latency. Furthermore, the maintain-017 ability /extensibility benefits of adapters can be 018 achieved with simpler approaches like multi-019 task training via full fine-tuning, which also 020 provide relatively faster training times. We, 021 therefore, recommend that for moderately sized 022 models practitioners should rely on full fine-023 tuning or multi-task training rather than using 024 adapters. 025"
}