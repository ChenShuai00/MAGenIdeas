{
    "id": "8b91fdbaed6754f18f1c2bae081750d12bfcd8e4",
    "title": "Improving Unsupervised Sentence Simplification Using Fine-Tuned Masked Language Models",
    "abstract": "Word suggestion in unsupervised sentence sim-001 plification is mostly done without consider-002 ing the context of the input sentence. Fortu-003 nately, masked language modeling is a well-004 established task for predicting the most suit-005 able candidate for a masked token using the 006 surrounding context words. In this paper, we 007 propose a technique that merges pre-trained 008 BERT models with a successful edit-based un-009 supervised sentence simplification model to 010 bring context-awareness into the simple word 011 suggestion functionality. Next, we show that 012 only by fine-tuning the BERT model on enough 013 simplistic sentences, simplification results can 014 be improved and even outperform some of the 015 competing supervised methods. Finally, we 016 introduce a framework that involves filtering 017 an arbitrary amount of unlabeled in-domain 018 texts for tuning the model. By removing use-019 less training samples, this preprocessing step 020 speeds up the fine-tuning process where labeled 021 data, as simple and complex, are scarce. 022"
}