{
    "id": "edd6dbf81aa54cc2ddacd0a23f1d862e28d5482a",
    "title": "How Good is a Recommender in Machine-Assisted Cross Document Event Coreference Resolution Annotation?",
    "abstract": "Annotating cross document event coreference 001 links is a tedious task that requires annotators 002 to have near-oracle knowledge of a document 003 collection. The heavy cognitive load of this 004 task decreases overall annotation quality while 005 inevitably increasing latency. To support anno-006 tation efforts, machine-assisted recommenders 007 can sample likely coreferent events for a given 008 target event, thus eliminating the burden of ex-009 amining large numbers of true negative pairs. 010 However, there has been little to no work in 011 evaluating the effectiveness of recommender 012 approaches, particularly for the task of event 013 coreference. To this end, we \ufb01rst create a sim-014 ulated version of recommender based annota-015 tion for cross document event coreference res-016 olution. Then, we adapt an existing method as 017 the model governing recommendations. And 018 \ufb01nally, we introduce a novel method to as-019 sess the simulated recommender by evaluating 020 an annotator-centric Recall-Annotation effort 021 tradeoff. 022"
}