{
    "id": "6995cfa818ddd1ba0b7eaf0564e4b080732f5000",
    "title": "Making Small Language Models Better Few-Shot Learners",
    "abstract": "Large-scale language models coupled with 001 prompts have shown remarkable performance 002 on few-shot learning. However, through sys-003 tematic experiments, we find that the few-shot 004 performance of small language models is poor, 005 and using prompts on them brings fewer im-006 provements than on larger ones. In this paper, 007 we propose SMASH , an approach to improve 008 SMA ll language models\u2019 few-SH ot ability by 009 training on intermediate tasks before prompt-010 based fine-tuning on downstream tasks. We de-011 sign intermediate tasks for sentence-pair tasks 012 and single-sentence classification tasks by cre-013 ating training examples with prompt templates 014 similar to downstream tasks using sentences 015 sampled from a large-scale unsupervised cor-016 pus, and apply knowledge distillation to distill 017 from outputs of larger pre-trained models as 018 training objective. We conduct extensive ex-019 periments and show that SMASH can make a 020 6-layer DistilRoBRETa-base achieve compa-021 rable performance on few-shot datasets to a 022 12-layer RoBERTa-base at a low cost. 1 023"
}