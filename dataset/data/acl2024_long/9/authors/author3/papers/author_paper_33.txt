{
    "id": "cdab2bd9b245eecb2ba0f61648e679c36ec6a30e",
    "title": "One Step Is Enough for Few-Shot Cross-Lingual Transfer: Co-Training with Gradient Optimization",
    "abstract": "The current state-of-the-art for few-shot cross-001 lingual transfer learning first trains on abundant 002 labeled data in the source language and then 003 fine-tunes with a few examples on the target 004 language, termed target-adapting . Though this 005 has been demonstrated to work on a variety of 006 tasks, in this paper we show some deficiencies 007 of this approach and propose a one-step co-008 training method that trains on both source and 009 target data with stochastic gradient surgery , a 010 novel gradient-level optimization. Unlike the 011 previous studies that focus on one language at 012 a time when target-adapting, we use one model 013 to handle all target languages simultaneously 014 to avoid excessively language-specific models. 015 Moreover, we discuss the unreality of utilizing 016 large target development sets for model selec-017 tion in previous literature, and further show 018 that our method is development-free for tar-019 get languages and also able to escape from 020 overfitting issues. We conduct a large-scale 021 experiment on 4 diverse NLP tasks across up to 022 48 languages. Our proposed method achieves 023 state-of-the-art performance on all tasks and 024 outperforms target-adapting by a large margin 1 , 025 especially for languages that are linguistically 026 distant from the source language, e.g., an aver-027 age of 7.36% absolute F1 improvement on the 028 NER task, up to a gain of 17.60% on Punjabi. 029"
}