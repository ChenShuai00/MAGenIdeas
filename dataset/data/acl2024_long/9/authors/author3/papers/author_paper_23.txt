{
    "id": "7edf292c9c6f0d7e704f03fd0ff910075ece1af7",
    "title": "Privacy, Interpretability, and Fairness in the Multilingual Space",
    "abstract": "Multilingual generalization or compression is 001 an objective for cross-lingual models in nat-002 ural language processing (NLP). We explore 003 how the compression sought for in such models 004 aligns with other common objectives in NLP 005 such as performance, differential privacy, inter-006 pretability, and fairness. We show that compres-007 sion, which can be quantified by, e.g., sentence 008 retrieval or centered kernel alignment, is com-009 patible with performance and privacy, but that 010 performance and privacy are at odds, leading to 011 non-linear interactions between compression, 012 performance, and privacy. We also demon-013 strate that privacy is at odds with interpretabil-014 ity, leading to non-linear interactions between 015 compression, privacy, and interpretability. Fi-016 nally, while fairness and privacy are generally 017 at odds, we show that in the multilingual space, 018 fairness and privacy have common solutions. In 019 sum, our study shows that if we want to learn 020 multilingual models that exhibit good perfor-021 mance and good generalization properties, and 022 are private, interpretable and fair (or any com-023 bination thereof), we need to jointly optimize 024 for these inter-dependent objectives. 1 025"
}