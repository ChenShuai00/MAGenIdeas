{
    "id": "34f1e90c4e73e05105b0b75fd72d8e7a7345fb74",
    "title": "SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering",
    "abstract": "Video question\u2013answering is a fundamental task in the field of video understanding. Though current state-of-the-art video\u2013language pretrained models have yielded appealing performance, they are at the cost of huge computational power and thus hard to deploy on many platforms with limited resources. An economical workaround simply samples a small portion of frames to tune an image\u2013text model on these sampled frames. However, the sampling methods adopted by these VLMs are quite simple and straightforward\u2014 such methods are aimless and often inevitably omit the key frames from which the correct answer can be deduced, and the situation becomes worse as the sampling sparsity increases, which particularly requires when the video lengths increase. To mitigate this issue, we propose two frame sampling strategies, namely the most dominant frames (MDF) and most implied frames (MIF), to maximally preserve those frames that are most likely vital to the given questions. MDF passively minimizes the risk of key frame omission in a bootstrap manner, while MIF actively searches key frames customized for each video\u2013question pair with the assistance of auxiliary models. The experimental results on three public datasets and three advanced VLMs (CLIP, GIT and All-in-one) demonstrate that our proposed strategies can boost the performance for image\u2013text pretrained models. The source codes pertaining to the method proposed in this paper are publicly available at https://github.com/declare-lab/sas-vqa."
}