{
    "id": "d6d068fad9785392c6007d4d6a74fdbf6e008fed",
    "title": "Graph-based Fine-grained Multimodal Attention Mechanism for Sentiment Analysis",
    "abstract": "Multimodal sentiment analysis is a popular 001 research area in natural language processing. 002 Mainstream multimodal learning models barely 003 consider that the visual and acoustic behaviors 004 often have a much higher temporal frequency 005 than words. Therefore, these models lack the 006 representation capability to accurately model 007 multimodal interactions. In this paper, we pro-008 pose an attachment called Graph-based Fine-009 grained Multimodal Attention Mechanism (GF-010 MAM), which can utilize the multimodal in-011 formation from different subspaces to achieve 012 accurate multimodal interactions. Firstly, the 013 attachment further splits the information of ev-014 ery modality into multiple subspaces. Then, the 015 fine-grained multimodal information from dif-016 ferent subspaces is converted into multimodal 017 interaction graphs dominant by the language 018 modality. The multimodal interaction graph 019 can capture significant interactions among mul-020 tiple modalities at the subspace level. Finally, 021 the information of nonverbal modalities is addi-022 tionally added to compensate for the loss of 023 continuity caused by the splitting operation. 024 Embedding GFMAM into BERT, we propose a 025 new model called GFMAM-BERT that can di-026 rectly accept nonverbal modalities in addition 027 to language modality. We conducted experi-028 ments on both publicly available multimodal 029 sentiment analysis datasets CMU-MOSI and 030 CMU-MOSEI. The experiment results demon-031 strate that GFMAM-BERT exceeds the state-of-032 the-art models. Moreover, the proposed model 033 outperforms humans on most metrics on the 034 CMU-MOSI dataset. 035"
}