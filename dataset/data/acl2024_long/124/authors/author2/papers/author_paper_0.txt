{
    "id": "04ebbac1a75b083ec871961b0e0807f5ac24393c",
    "title": "INSTRAUG: Automatic Instruction Augmentation for Multimodal Instruction Fine-tuning",
    "abstract": "Fine-tuning large language models (LLMs) on multi-task instruction-following data has been proven to be a powerful learning paradigm for improving their zero-shot capabilities on new tasks. Recent works about high-quality instruction-following data generation and selection require amounts of human labor to conceive model-understandable instructions for the given tasks and carefully filter the LLM-generated data. In this work, we introduce an automatic instruction augmentation method named I NSTR A UG in multimodal tasks. It starts from a handful of basic and straight-forward meta instructions but can expand an instruction-following dataset by 30 times. Re-sults on two popular multimodal instruction-following benchmarks M ULTI I NSTRUCT and InstructBLIP show that I NSTR A UG can significantly improve the alignment of multi-modal large language models (MLLMs) across 12 multimodal tasks, which is even equivalent to the benefits of scaling up training data multiple times. The code and datasets are available at https://github.com/ declare-lab/InstrAug ."
}