{
    "id": "57c84efbeebd8bdfc66f15eb6b7fcc24a117dcbf",
    "title": "Born Again Neural Rankers",
    "abstract": "We introduce Born Again neural Rankers (BAR) in the Learning to Rank (LTR) setting, where student rankers, trained in the Knowledge Distillation (KD) framework, are parameterized identically to their teachers. Unlike the existing ranking distillation work which pursues a good trade-o\ufb00 between performance and e\ufb03ciency, BAR adapts the idea of Born Again Networks (BAN) to ranking problems and signi\ufb01cantly improves ranking performance of students over the teacher rankers without increasing model capacity. The key di\ufb00erences between BAR and common distillation techniques for classi-\ufb01cation are: (1) an appropriate teacher score transformation function, and (2) a novel listwise distillation framework. Both techniques are speci\ufb01cally designed for ranking problems and are rarely studied in the knowledge distillation literature. Using the state-of-the-art neural ranking structure, BAR is able to push the limits of neural rankers above a recent rigorous benchmark study and signi\ufb01cantly outperforms traditionally strong gradient boosted decision tree based models on 7 out of 9 key metrics, the \ufb01rst time in the literature. In addition to the strong empirical results, we give theoretical explanations on why listwise distillation is e\ufb00ective for neural rankers."
}