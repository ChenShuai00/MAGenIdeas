{
    "id": "760561c57f68044e2f1d089088df1da6c627b09a",
    "title": "On the Advance of Making Language Models Better Reasoners",
    "abstract": "Large language models such as GPT-3 and PaLM have shown remarkable performance in few-shot learning. However, they still struggle with reasoning tasks such as the arithmetic benchmark GSM8K. Recent advances deliberately guide the language model to generate a chain of reasoning steps before producing the \ufb01nal answer, successfully boosting the GSM8K benchmark from 17 . 9% to 58 . 1% in terms of problem solving rate. In this paper, we propose a new approach, D I V E RS E (Diverse Veri\ufb01er on Reasoning Step), to further advance their reasoning capability. D I V E RS E \ufb01rst explores different prompts to enhance the diversity in reasoning paths. Second, D I V E RS E introduces a veri\ufb01er to distinguish good answers from bad answers for a better weighted voting. Finally, D I V E RS E veri\ufb01es the correctness of each single step rather than all the steps in a whole. We conduct extensive experiments using the latest language model code-davinci-002 and demonstrate that D I - V E RS E can achieve new state-of-the-art performance on six out of eight reasoning benchmarks (e.g., GSM8K 74 . 4% \u2192 83 . 2% ), out-performing the PaLM model with 540B parameters."
}