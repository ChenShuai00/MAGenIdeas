{
    "id": "75c68b6fb45e6f41f8e6419414e474da445ea36d",
    "title": "Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy",
    "abstract": "Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data and aids in the development of countermeasures. Many prior works\u2014and some recently deployed defenses\u2014focus on \u201cverbatim memorization\u201d, defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense that _perfectly_ prevents all verbatim memorization. And yet, we demonstrate that this \u201cperfect\u201d filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified \u201cstyle-transfer\u201d prompts\u2014and in some cases even the non-modified original prompts\u2014to extract memorized information. We conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models."
}