{
    "id": "9e49531fea2459deb5c466722417945770377f58",
    "title": "The Detection of Machine Generated Text",
    "abstract": "One of the oldest yet most elusive promises of AI is computers that can converse with humans, not just via rigidly structured templates and programming languages, but in natural language. This problem is formalized as Natural Language Generation (NLG), the task of writing novel sentences in a human language such as English. When I first began conducting research on NLG, my goal was to explore open-ended tasks, such as dialog and story writing, where a human and AI system interact to write text. However, I quickly ran into the problem that machine-generated text was frustratingly insufficient for my ambitions. Though state-of-the-art NLG systems can now produce text that, at a surface-level, is fluent and grammatical, generated text still differs in significant ways from the text humans would write. The issues range from common-sense errors most people easily pick up on, like writing about a four-horned unicorn, to subtle statistical anomalies, like using the word \"the\" 25% more frequently than human-written text. I realized that if I am to advance the applications of NLG, I need to simultaneously pursue a deeper understanding of its limitations and methods for mitigation. In my work as a PhD student at the University of Pennsylvania and a researcher with Google Brain, I have developed a research program focused on advancing state-of-the-art NLG while pursuing thoughtful analysis of its limitations and ramifications. For example, I have found that minor changes to the algorithms used to generate text can significantly impact the manner in which it deviates from genuine human text. I have also measured how the neural networks used for NLG have a worrying tendency to memorize and regurgitate entire passages from their training data, including personal conversations and copyrighted book chapters. Finally, I have worked on extending NLG's capabilities to serve as a collaborative partner in creative writing applications, a domain where the aforementioned limitations I've researched have significant consequences. The study of NLG has taken on surprisingly broad societal importance as researchers rush toward bigger, more powerful models and practitioners incorporate language generation into increasingly far-flung applications, from text adventure games and educational tools to email and code writing assistants. With the expertises that I gained at UPenn and at Google Brain, I am well positioned to help lead this emerging subfield of artificial intelligence."
}