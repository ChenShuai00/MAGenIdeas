{
    "id": "c61459f2ddf053a2768105c00a020ec4a81b1091",
    "title": "TILGAN: Transformer-based Implicit Latent GAN for Diverse and Coherent Text Generation",
    "abstract": "Conventional autoregressive models have achieved great success in text generation but suffer from the exposure bias problem in that token sequences in the training and in the generation stages are mismatched. While generative adversarial networks (GANs) can remedy this problem, existing implementations of GANs directly on discrete outputs tend to be unstable and lack diversity. In this work, we propose TILGAN , a T ransformer-based I mplicit L atent GAN , which combines a Transformer autoencoder and GAN in the latent space with a novel design and distribution matching based on the Kullback-Leibler (KL) divergence. Speci\ufb01cally, to improve local and global coherence, we explicitly introduce a multi-scale discriminator to capture the semantic information at varying scales among the sequence of hidden representations encoded by Transformer. Moreover, the decoder is enhanced by an additional KL loss to be consistent with the latent-generator. Experimental re-sults on three benchmark datasets demonstrate the validity and effectiveness of our model, by obtaining signi\ufb01cant improvements and a better quality-diversity trade-off in automatic and human evaluation for both unconditional and conditional generation tasks. 1"
}