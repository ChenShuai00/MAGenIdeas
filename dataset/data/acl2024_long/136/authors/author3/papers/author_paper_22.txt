{
    "id": "6c5525f8747ecf27908d71703902eb7374ff0a99",
    "title": "Handling and Presenting Harmful Text",
    "abstract": "Textual data can pose a risk of serious harm. These harms can be categorised along three axes: (1) the harm type (e.g. misinformation, hate speech or racial stereotypes) (2) whether it is elicited as a feature of the research de-sign from directly studying harmful content (e.g. training a hate speech classi\ufb01er or auditing un\ufb01ltered large-scale datasets) versus spuriously invoked from working on unrelated problems (e.g. language generation or part of speech tagging) but with datasets that nonethe-less contain harmful content, and (3) who it affects, from the humans (mis)represented in the data to those handling or labelling the data to readers and reviewers of publications produced from the data. It is an unsolved problem in NLP as to how textual harms should be handled, presented, and discussed; but, stop-ping work on content which poses a risk of harm is untenable. Accordingly, we provide practical advice and introduce H ARM C HECK , a resource for re\ufb02ecting on research into textual harms. We hope our work encourages ethical, responsible, and respectful research in the NLP community."
}