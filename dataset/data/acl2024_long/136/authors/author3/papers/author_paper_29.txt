{
    "id": "368396090880716454457db74659b430dce5345b",
    "title": "How True is GPT-2? An Empirical Analysis of Intersectional Occupational Biases",
    "abstract": "The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Downstream applications are at risk of inheriting biases contained in these models, with potential negative consequences especially for marginalized groups. In this paper, we analyze the occupational biases of a popular generative language model, GPT-2, intersecting gender with \ufb01ve protected categories: religion, sexuality, ethnicity, political af\ufb01liation, and name origin. Using a novel data collection pipeline we collect 396k sentence completions of GPT-2 and \ufb01nd: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Fitting 262 logistic models shows intersec-tional interactions to be highly relevant for occupational associations; (iii) For a given job, GPT-2 re\ufb02ects the societal skew of gender and ethnicity in the US, and in some cases, pulls the distribution towards gender parity, raising the normative question of what language models should learn. Code is available at https://github.com/ oxai/intersectional_gpt2 ."
}