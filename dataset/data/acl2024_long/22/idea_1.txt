{
    "Title": "Meta-Evaluation of MT Metrics: A Large-Scale Study on Metric Robustness and Interpretability",
    "Idea": "This paper conducts a large-scale meta-evaluation of MT metrics, focusing on their robustness to linguistic perturbations, interpretability, and correlation with human judgments. Using the DEMETR dataset, the study evaluates metrics like BLEU, METEOR, COMET, and BLEURT across 35 linguistic error categories (e.g., semantic, syntactic, morphological). The paper introduces a new interpretability score for metrics, which measures how well their outputs can be traced back to specific translation errors. The study also explores the impact of reference quality on metric performance and proposes a method for generating synthetic references to mitigate bad reference issues. The findings will provide actionable insights for metric developers and users, helping them choose the most appropriate metrics for specific tasks.",
    "Thinking": "This idea is inspired by **Hansen’s theory of anomalous findings** (explaining why some metrics perform poorly on certain error categories) and **Popper’s falsificationism** (critically analyzing the limitations of existing metrics). The rationale is that while many metrics claim to outperform BLEU, their robustness and interpretability are not well understood. This study aims to fill this gap by providing a comprehensive evaluation framework.",
    "Rationale": "The rationale for this idea is that the MT community lacks a clear understanding of which metrics are robust and interpretable across different error types and reference qualities. By addressing this, the study can guide future metric development and improve the credibility of MT evaluations."
}