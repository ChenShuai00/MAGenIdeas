{
    "id": "36d448c6c9a3e27e9172dacc9bc3823f9edbac8a",
    "title": "Efficiently Reusing Old Models Across Languages via Transfer Learning",
    "abstract": "Recent progress in neural machine translation (NMT) is directed towards larger neural networks trained on an increasing amount of hardware resources. As a result, NMT models are costly to train, both financially, due to the electricity and hardware cost, and environmentally, due to the carbon footprint. It is especially true in transfer learning for its additional cost of training the \u201cparent\u201d model before transferring knowledge and training the desired \u201cchild\u201d model. In this paper, we propose a simple method of re-using an already trained model for different language pairs where there is no need for modifications in model architecture. Our approach does not need a separate parent model for each investigated language pair, as it is typical in NMT transfer learning. To show the applicability of our method, we recycle a Transformer model trained by different researchers and use it to seed models for different language pairs. We achieve better translation quality and shorter convergence times than when training from random initialization."
}