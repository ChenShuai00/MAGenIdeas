{
    "id": "10bb3bb91db727364c41b272a0b015376a05c827",
    "title": "How to Do Human Evaluation: Best Practices for User Studies in NLP",
    "abstract": "Many research topics in natural language pro-001 cessing (NLP), such as explanation genera-002 tion, dialog modeling or machine translation, 003 require evaluation that goes beyond standard 004 metrics like accuracy or F 1 score toward a 005 more human-centered approach. Therefore, 006 understanding how to design user studies be-007 comes increasingly important. However, few 008 comprehensive resources exist on planning, 009 conducting and evaluating user studies for 010 NLP, making it hard to get started for re-011 searchers without prior experience in the \ufb01eld 012 of human evaluation. In this paper, we summa-013 rize the most important aspects of user studies 014 and their design and evaluation, providing di-015 rect links to NLP tasks and NLP speci\ufb01c chal-016 lenges where appropriate. We (i) outline gen-017 eral study design, ethical considerations, and 018 factors to consider for crowdsourcing, (ii) dis-019 cuss the particularities of user studies in NLP 020 and provide starting points to select question-021 naires, experimental designs and evaluation 022 methods that are tailored to the speci\ufb01c NLP 023 tasks. Additionally, we offer examples with 024 accompanying statistical evaluation code in R 025 throughout, to bridge the gap between theoret-026 ical guidelines and practical applications. 1 027"
}