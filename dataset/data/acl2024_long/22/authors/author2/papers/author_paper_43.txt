{
    "id": "93a3a8b6325ca9e26622f0780e1626e36fd934d5",
    "title": "Applying cross-entropy difference for selecting parallel training data from publicly available sources for conversational machine translation",
    "abstract": "Cross Entropy Difference (CED) has proven to be a very effective method for selecting domain-specific data from large corpora of out-of-domain or general domain content. It is used in a number of different scenarios, and is particularly popular in bake-off competitions in which participants have a limited set of resources to draw from, and need to sub-sample the data in such a way as to ensure better results on domainspecific test sets. The underlying algorithm is handy since one can provide a set of in-domain data and, using a language model (LM) trained on this in-domain data, along with one trained on out-of-domain or general domain content, use it to \u201cidentify more of the same.\u201d Although CED was designed to select domain-specific data, in this work we are generous regarding the notion of \u201cdomain\u201d. Instead of looking for data of a particular domain, we seek to identify data of a particular style, specifically, data that is conversational. Our interest is to train conversational Machine Translation (MT) systems, and boost the available data using CED against large, publicly available general domain corpora. Experimental results on conversational test sets show that CED can greatly benefit machine translation system quality in conversational scenarios, and can be used to significantly increase the amount of parallel conversational data available."
}