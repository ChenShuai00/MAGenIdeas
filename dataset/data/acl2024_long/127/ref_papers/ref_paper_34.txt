{
    "id": "259d07f47c046389d0b4f256139de64736dd9a94",
    "title": "Jailbreaker: Automated Jailbreak Across Multiple Large Language Model Chatbots",
    "abstract": "\u2014The landscape of Arti\ufb01cial Intelligence (AI) services has been signi\ufb01cantly in\ufb02uenced by the rapid proliferation of Large Language Models (LLMs), primarily due to their remarkable pro\ufb01ciency in comprehending, generating, and completing text in a manner that mirrors human interaction. Among these services, LLM-based chatbots have gained widespread popularity due to their ability to facilitate smooth and intuitive human-machine exchanges. However, their susceptibility to jailbreak attacks \u2014 attempts by malicious users to prompt sensitive or harmful responses against service guidelines \u2014 remains a critical concern. Despite numerous efforts to expose these weak points, our research presented in this paper indicates that current strategies fall short in effectively targeting mainstream LLM chatbots. This ineffectiveness can be largely attributed to undisclosed defensive measures, implemented by service providers to thwart such exploitative attempts. Our paper presents J AILBREAKER , a comprehensive framework that offers insight into the intriguing dynamics of jail-break attacks and the countermeasures deployed against them. J AILBREAKER provides a dual-pronged contribution. Initially, we propose a novel method that utilizes time-based characteristics intrinsic to the generation process to deconstruct the defense mechanisms employed by popular LLM chatbot services. This approach, informed by time-based SQL injection techniques, allows us to unravel valuable details about the functioning of these defensive measures. Through careful manipulation of the chatbots\u2019 time-sensitive reactions, we unravel the complex aspects of their design and establish a proof-of-concept attack to circumvent the defenses of multiple LLM chatbots such as C HAT GPT, Bard, and Bing Chat. Our second offering is an innovative method for the automatic generation of jailbreak prompts that target robustly defended LLM chatbots. The crux of this approach involves leveraging an LLM to auto-learn successful patterns. By \ufb01ne-tuning an LLM with jailbreak prompts, we validate the potential of automated jailbreak creation for several high-pro\ufb01le commercial LLM chatbots. Our method generates attack prompts achieving an average success rate of 21.58%, considerably surpassing the 7.33% success rate accomplished by existing prompts. We have conscientiously reported our \ufb01ndings to the impacted service providers. J AILBREAKER establishes a groundbreaking approach to unveil vulnerabilities in LLMs, underscoring the need for more formidable defenses against such intrusions."
}