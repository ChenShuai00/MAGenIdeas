{
    "Title": "Emotional Manipulation as a Jailbreaking Vector: Defending LLMs Against Affective Adversarial Prompts",
    "Idea": "This research explores how emotional manipulation—such as appeals to sympathy, fear, or guilt—can be used to jailbreak LLMs. We will: (1) develop a taxonomy of emotional manipulation techniques derived from psychology literature, (2) create a dataset of affective adversarial prompts (AAP) that exploit these techniques, (3) evaluate the effectiveness of AAP across multiple LLMs, and (4) design a defense mechanism that detects and neutralizes emotionally manipulative prompts by analyzing linguistic and contextual cues. The defense will integrate psychological principles with machine learning to identify and block affective attacks while preserving model utility.",
    "Thinking": "The idea is grounded in Kuhn’s paradigm theory (shifting the view of LLMs as social entities) and Pierce’s hypothetical deduction (proposing emotional manipulation as a new jailbreaking vector). It also leverages Laudan’s methodological improvement model to design a defense that combines psychological insights with technical solutions. The interdisciplinary nature of the work (AI safety + psychology) and its potential to uncover a new class of vulnerabilities make it highly innovative and impactful.",
    "Rationale": "Current jailbreaking research focuses on logical or algorithmic attacks (e.g., adversarial suffixes, cipher attacks), overlooking the role of human emotions in persuasion. Emotional manipulation is a potent real-world tactic, and its application to LLMs is unexplored. By addressing this gap, the research could: (1) reveal a new class of vulnerabilities, (2) provide a robust defense mechanism, and (3) inspire future work on human-centric AI safety. The combination of novelty, practical impact, and interdisciplinary rigor positions this idea for top-tier publications."
}