{
    "id": "8c5b0765e960cc77b38b1e9f930726da01597364",
    "title": "Improving Few-shot Text Classification via Pretrained Language Representations",
    "abstract": "Text classi\ufb01cation tends to be dif\ufb01cult when the data is de\ufb01cient or when it is required to adapt to unseen classes. In such challenging scenarios, recent studies have often used meta-learning to simulate the few-shot task, thus negating explicit common linguistic features across tasks. Deep language representations have proven to be very effective forms of unsupervised pretraining, yielding contextualized features that capture linguistic properties and bene\ufb01t downstream natural language understanding tasks. However, the effect of pretrained language representation for few-shot learning on text classi\ufb01cation tasks is still not well understood. In this study, we design a few-shot learning model with pretrained language representations and report the empirical results. We show that our approach is not only simple but also produces state-of-the-art performance on a well-studied sentiment classi\ufb01cation dataset. It can thus be further suggested that pretraining could be a promising solution for few shot learning of many other NLP tasks. The code and the dataset to replicate the experiments are made available at https"
}