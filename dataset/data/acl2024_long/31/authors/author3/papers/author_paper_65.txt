{
    "id": "df25192774efe751e8be1f12067e6a6ac60bffb1",
    "title": "LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER",
    "abstract": "NER in low-resource domains suffers from insuf\ufb01cient training data. Existing transfer learning approaches for low-resource NER usually have the challenge that the target domain has different sets of entity categories compared with a resource-rich source domain, which can be concluded as class transfer and domain transfer problems. In this paper, we pro-pose a lightweight generative framework with prompt-guided attention for low-resource NER (LightNER). Concretely, instead of tackling the problem by training label-speci\ufb01c discriminative classi\ufb01ers, we convert sequence labeling to generate the entity pointer index sequence and entity categories without any label-speci\ufb01c classi\ufb01ers, which can address the class transfer issue. We further propose prompt-guided attention by incorporating continuous prompts into the self-attention layer to re-modulate the attention and adapt pre-trained weights. Note that we only tune those continuous prompts with the whole parameter of the pre-trained language model \ufb01xed, thus, making our approach lightweight and \ufb02ex-ible for low-resource scenarios and can better transfer knowledge across domains. Experimental results show that by tuning only 0.16% of the parameters, LightNER can obtain comparable performance in the standard setting and outperform baselines in low-resource settings."
}