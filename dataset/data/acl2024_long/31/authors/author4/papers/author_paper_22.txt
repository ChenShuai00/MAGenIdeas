{
    "id": "11fe3462c627c59093c71aac81e91a7dce8554de",
    "title": "CLoCE:Contrastive Learning Optimize Continous Prompt Embedding Space in Relation Extraction",
    "abstract": "Recent studies have proved that prompt tuning 001 can improve the performance of pre-trained lan-002 guage models on downstream tasks. However, 003 in the task of relation extraction (RE), there are 004 still a large number of confusing samples that 005 hinder prompt-tuning method from achieving 006 higher accuracy. Inspired by previous works, 007 we innovatively utilize contrastive learning to 008 solve this problem. We propose a prompt-009 tuning-based framework and apply contrastive 010 learning to optimize the representation of in-011 put sentences in embedding space. At the same 012 time, we design a more general template for RE 013 task, and further use knowledge injection to im-014 prove performance of the model. Through ex-015 tensive experiments on public datasets, the mi-016 cro F 1 -score(%) of our model exceeds the ex-017 isting SOTA on the Re-TACRED and TACREV 018 datasets by 0.5 and 1.0, respectively. Mean-019 while, in the few-shot scenario, our model also 020 has a more robust performance than fine-tune 021 methods. 022"
}