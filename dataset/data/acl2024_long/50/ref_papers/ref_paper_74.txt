{
    "id": "0505f17c4052366cbc4fad99150d3542edf85faa",
    "title": "TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer",
    "abstract": "The problem of \ufb01xing errors in programs has at-tracted substantial interest over the years. The key challenge for building an effective code \ufb01x-ing tool is to capture a wide range of errors and meanwhile maintain high accuracy. In this paper, we address this challenge and present a new learning-based system, called TFix. TFix works directly on program text and phrases the problem of code \ufb01xing as a text-to-text task. In turn, this enables it to leverage a powerful Transformer based model pre-trained on natural language and \ufb01ne-tuned to generate code \ufb01xes (via a large, high-quality dataset obtained from GitHub commits). TFix is not speci\ufb01c to a particular programming language or class of defects and, in fact, improved its precision by simultaneously \ufb01ne-tuning on 52 different error types reported by a popular static analyzer. Our evaluation on a massive dataset of JavaScript programs shows that TFix is practically effective: it is able to synthesize code that \ufb01xes the error in \u223c 67 percent of cases and signi\ufb01cantly outperforms existing learning-based approaches."
}