{
    "id": "04cab71e7ff997d32332a633d17abbaa54589764",
    "title": "AudioTagging Done Right: 2nd comparison of deep learning methods for environmental sound classification",
    "abstract": "After its sweeping success in vision and language tasks, pure attention-based neural architectures (e.g. DeiT) [1] are emerg-ing to the top of audio tagging (AT) leaderboards [2], which seemingly obsoletes traditional convolutional neural networks (CNNs), feed-forward networks or recurrent networks. How-ever, taking a closer look, there is great variability in pub-lished research, for instance, performances of models initialized with pretrained weights differ drastically from without pretraining [2], training time for a model varies from hours to weeks, and often, essences are hidden in seemingly trivial details. This urgently calls for a comprehensive study since our 1st comparison [3] is half-decade old. In this work, we perform extensive experiments on AudioSet [4] which is the largest weakly-labeled sound event dataset available, we also did analysis based on the data quality and efficiency. We compare a few state-of-the-art baselines on the AT task, and study the performance and efficiency of 2 major categories of neural architectures: CNN variants and attention-based variants. We also closely examine their optimization procedures. Our open-sourced experimental results 1 provide insights to trade off between performance, efficiency, optimization process, for both practitioners and researchers."
}