{
    "Title": "TruthProbe: Unsupervised Discovery of Truthful Directions in LLM Activations",
    "Idea": "This idea introduces TruthProbe, an unsupervised method for discovering truthful directions in LLM activations. Unlike supervised approaches that require labeled data, TruthProbe leverages logical consistency properties (e.g., a statement and its negation should have opposite truth values) to identify directions in activation space that correlate with truthfulness. The method involves training a lightweight probe using contrastive learning on unlabeled model activations, guided by logical constraints. During inference, TruthProbe can be used to detect and correct hallucinations by steering LLM outputs toward truthful directions. Experiments will show that TruthProbe outperforms supervised baselines on TruthfulQA and generalizes to unseen domains without additional fine-tuning.",
    "Thinking": "This idea is grounded in **Hansen’s theory of anomalous findings** (Law 8), which focuses on explaining and integrating anomalous results. The observation that LLMs often generate falsehoods despite knowing the truth is an anomaly that TruthProbe aims to address. The method is inspired by **Pierce’s hypothetical deduction method** (Law 2), using logical consistency as a guiding principle for hypothesis formation. Additionally, **Laudan’s methodological improvement model** (Law 4) is applied to design a novel probing technique that improves upon existing supervised methods by eliminating the need for labeled data.",
    "Rationale": "The rationale for TruthProbe stems from the limitations of supervised methods, which require extensive labeled data and may not generalize well to new domains. Papers like 'Discovering Latent Knowledge in Language Models Without Supervision' and 'The Internal State of an LLM Knows When it’s Lying' suggest that LLMs internally represent truthfulness, even when their outputs are false. By leveraging logical consistency, TruthProbe provides a scalable and domain-agnostic solution for improving LLM truthfulness. This approach is particularly relevant for applications where labeled data is scarce, such as low-resource languages or specialized domains."
}