{
    "id": "541879410a46fff2d9c92bbf7f7787d4d3f8d11a",
    "title": "Gapformer: Graph Transformer with Graph Pooling for Node Classification",
    "abstract": "Graph Transformers (GTs)\u00a0have proved their advantage in graph-level tasks. However, existing GTs still perform\u00a0unsatisfactorily on the node classification task due to\u00a01) the overwhelming unrelated information obtained from a vast number of irrelevant distant nodes and 2) the quadratic complexity regarding the number of nodes via the fully connected attention mechanism. In this paper, we present Gapformer, a method for node classification that deeply incorporates Graph Transformer with Graph Pooling. More specifically, Gapformer coarsens the large-scale nodes of a graph into a smaller number of pooling nodes via local or global graph pooling methods, and then computes the attention solely with the pooling nodes rather than all other nodes. In such a manner, the negative influence of the overwhelming unrelated nodes is mitigated while maintaining the long-range information, and the quadratic complexity is reduced to linear complexity with respect to the fixed number of pooling nodes. Extensive experiments on 13 node classification datasets, including homophilic and heterophilic graph datasets, demonstrate the\u00a0competitive performance of\u00a0Gapformer over\u00a0existing Graph Neural Networks and GTs."
}