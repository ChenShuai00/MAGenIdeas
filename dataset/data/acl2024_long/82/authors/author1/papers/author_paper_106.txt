{
    "id": "c4eba54af99405d0b8f20faaba3fcc3c0efb9ec5",
    "title": "SLUA: A Super Lightweight Unsupervised Word Alignment Model via Cross-Lingual Contrastive Learning",
    "abstract": "Word alignment is essential for the down-streaming cross-lingual language understanding and generation tasks. Recently, the performance of the neural word alignment models [Zenkel et al. , 2020; Garg et al. , 2019; Ding et al. , 2019 ] has exceeded that of statistical models. However, they heavily rely on sophisticated translation models. In this study, we propose a Super Lightweight Unsupervised word Alignment ( SLUA ) model, in which a bidirectional symmetric attention trained with a contrastive learning objective is introduced, and an agreement loss is employed to bind the attention maps, such that the alignments follow mirror-like symmetry hypothesis. Experimental re-sults on several public benchmarks demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in word alignment while signi\ufb01cantly reducing the training and decoding time on average. Further ablation analysis and case studies show the superiority of our proposed SLUA. Notably, we recognize our model as a pioneer attempt to unify bilingual word embedding and word alignments. En-couragingly, our approach achieves 16.4 \u00d7 speedup against GIZA++, and 50 \u00d7 parameter compression compared with the Transformer-based alignment methods. We will release our code to facilitate the community."
}