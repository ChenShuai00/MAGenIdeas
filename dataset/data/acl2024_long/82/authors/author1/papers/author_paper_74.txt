{
    "id": "4141f53e7892120c67eafe561efaacdb0156b6e1",
    "title": "Cherry Hypothesis: Identifying the Cherry on the Cake for Dynamic Networks",
    "abstract": "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model\u2019s representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. Recent studies empirically show the trend that the more dynamic layers contribute to ever-increasing performance. However, such a fully dynamic setting 1) may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models, and more importantly, 2) contradicts the previous discovery in the human brain that when human brains process an attention-demanding task, only partial neurons in the task-speci\ufb01c areas are activated by the input, while the rest neurons leave in a baseline state. Critically, there is no e\ufb00ort to understand and resolve the above contradictory \ufb01nding, leaving the primal question \u2013 whether to make the computational parameters fully dynamic or not? \u2013 unanswered. The main contributions of our work are challenging the basic commonsense in dynamic networks, and, proposing and validating the cherry hypothesis \u2013 A fully dynamic network contains a subset of dynamic parameters that when transforming other dynamic parameters into static ones, can maintain or even exceed the performance of the original network. Technically, we propose a brain-inspired partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative"
}