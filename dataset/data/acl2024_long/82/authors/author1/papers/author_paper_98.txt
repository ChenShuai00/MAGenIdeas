{
    "id": "4f71fd26be06f479099b17af95d408c7ab272830",
    "title": "Improving Neural Machine Translation by Bidirectional Training",
    "abstract": "We present a simple and effective pretraining strategy \u2013 bidirectional training (BiT) for neural machine translation. Specifically, we bidirectionally update the model parameters at the early stage and then tune the model normally. To achieve bidirectional updating, we simply reconstruct the training samples from \u201csrc\\rightarrowtgt\u201d to \u201csrc+tgt\\rightarrowtgt+src\u201d without any complicated model modifications. Notably, our approach does not increase any parameters or training steps, requiring the parallel data merely. Experimental results show that BiT pushes the SOTA neural machine translation performance across 15 translation tasks on 8 language pairs (data sizes range from 160K to 38M) significantly higher. Encouragingly, our proposed model can complement existing data manipulation strategies, i.e. back translation, data distillation, and data diversification. Extensive analyses show that our approach functions as a novel bilingual code-switcher, obtaining better bilingual alignment."
}