{
    "id": "2294e237aa46d3bb34f71d224a6da9bdf061329b",
    "title": "Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender",
    "abstract": "Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting ( IAPrompt ). The principle behind is to trigger LLMs\u2019 inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness 1 . Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harm-fulness in response (averagely -46.5% attack success rate) and maintain the general help-fulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alphadl/ SafeLLM with IntentionAnalysis"
}