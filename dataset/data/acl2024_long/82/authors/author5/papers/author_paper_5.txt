{
    "id": "92512a21c09822037da980fded9e485d0d00e677",
    "title": "Is Encoded Popularity Always Harmful? Explicit Debiasing with Augmentation for Contrastive Collaborative Filtering",
    "abstract": "Collaborative Filtering (CF) models based on Graph Contrastive Learning (GCL) have effectively improved the performance of long-tail recommendation. However, the popularity bias still presents a challenge in further enhancing their effectiveness. Some studies suggest that achieving better recommendations, particularly for the long-tail, requires learning representations with a more uniform distribution to implicitly mitigate popularity bias. Nevertheless, our analysis of various CF models reveals that different models exhibit varying abilities in capturing popularity and those with superior performance might encode more popularity information in item representations. This raises a question: Does encoding popularity always lead to harmful bias? We speculate that superior recommendations may emerge from leveraging the encoded popularity information to optimize the representations of users and items rather than eliminating its existence in representations. This motivates a data augmentation approach, wherein we generate augmented samples by mixing representations of items with different popularity levels and explicitly debias using the encoded popularity information which is often neglected. Additionally, we propose an adaptive contrastive loss, leveraging structural information and unifying the recommendation and contrastive learning objectives, which adaptively re-weights positive samples and ensures the capture of item popularity. Our proposed framework remains scalable without requiring multiple forward computations throughout the entire graph. Extensive experiments demonstrate improvements in both overall and long-tail recommendation performance."
}