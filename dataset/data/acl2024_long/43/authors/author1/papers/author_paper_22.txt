{
    "id": "7db7fb25a8753c9efc6b3722d178f94fcc1f82d3",
    "title": "KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP",
    "abstract": "This paper focuses on text data augmentation for few-shot NLP tasks. The existing data augmentation algorithms either leverage task-independent heuristic rules (e.g., Synonym Replacement) or \ufb01ne-tune general-purpose pre-trained language models (e.g., GPT2) using a small training set to produce new synthetic data. Consequently, these methods have trivial task-speci\ufb01c knowledge and are limited to yielding low-quality synthetic data for weak baselines in simple tasks. To combat this issue, we propose Know ledge Mixture D ata A ugmentation Model ( KnowDA ): an encoder-decoder LM pretrained on a mixture of diverse NLP tasks using K nowledge M ixture T raining (KoMT). KoMT is a training procedure that reformulates input examples from various heterogeneous NLP tasks into a uni\ufb01ed text-to-text format, and employs denoising objectives in different granularity to learn to generate partial or complete samples. With the aid of KoMT, KnowDA could combine required task-speci\ufb01c knowledge implicitly from the learned mixture of tasks and quickly grasp the inherent synthesis law of the target task through a few given instances. To the best of our knowledge, we are the \ufb01rst attempt to scale the number of tasks to 100+ in multi-task co-training for data augmentation. Extensive experiments show that i) KnowDA successfully improves the performance of Albert and Deberta by a large margin on the FewGLUE benchmark, outperforming previous state-of-the-art data augmentation baselines; ii) KnowDA could for data augmentation in few-shot language learning for the \ufb01rst time. We demonstrate that the proposed Knowledge Mixture enables pre-trained language models the capability of generating proper synthetic instances from scratch for complicated tasks (i.e., the data sample has long sequences or multiple sentences). the effectiveness of our and KnowDA outperforms state-of-the-art"
}