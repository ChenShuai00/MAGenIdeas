{
    "id": "165d51a547cd920e6ac55660ad5c404dcb9562ed",
    "title": "Open Sesame: Getting inside BERT\u2019s Linguistic Knowledge",
    "abstract": "How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT\u2019s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT\u2019s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT\u2019s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora."
}