{
    "id": "b3bfe09a1156d33de75f584d4682bf5863845e8e",
    "title": "One Size Fits All? A simple LSTM for non-literal token and construction-level classification",
    "abstract": "In this paper, we tackle four different tasks of non-literal language classification: token and construction level metaphor detection, classification of idiomatic use of infinitive-verb compounds, and classification of non-literal particle verbs. One of the tasks operates on the token level, while the three other tasks classify constructions such as \u201chot topic\u201d or \u201cstehen lassen\u201d (\u201cto allow sth. to stand\u201d vs. \u201cto abandon so.\u201d). The two metaphor detection tasks are in English, while the two non-literal language detection tasks are in German. We propose a simple context-encoding LSTM model and show that it outperforms the state-of-the-art on two tasks. Additionally, we experiment with different embeddings for the token level metaphor detection task and find that 1) their performance varies according to the genre, and 2) word2vec embeddings perform best on 3 out of 4 genres, despite being one of the simplest tested model. In summary, we present a large-scale analysis of a neural model for non-literal language classification (i) at different granularities, (ii) in different languages, (iii) over different non-literal language phenomena."
}