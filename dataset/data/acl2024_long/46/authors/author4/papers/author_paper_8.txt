{
    "id": "41468e8c399776b157b4e077a5e28e037b131323",
    "title": "Like a Good Nearest Neighbor: Practical Content Moderation with Sentence Transformers",
    "abstract": "Text classification systems have impressive ca-001 pabilities but are infeasible to deploy and use 002 reliably due to their dependence on prompting 003 and billion-parameter language models. Set-004 Fit (Tunstall et al., 2022) is a recent, practi-005 cal approach that fine-tunes a Sentence Trans-006 former under a contrastive learning paradigm 007 and achieves similar results to more unwieldy 008 systems. Text classification is important for ad-009 dressing the problem of domain drift in detect-010 ing harmful content, which plagues social me-011 dia platforms. Here, we propose Like a Good 012 Nearest Neighbor (L A G O NN), a modification 013 to SetFit that requires no additional parameters 014 or hyperparameters but alters input text with 015 information from its nearest neighbor, for ex-016 ample, the label and text, in the training data, 017 making novel data appear similar to an instance 018 on which the model was optimized. L A G O NN 019 is effective at identifying harmful content and 020 generally improves SetFit\u2019s performance. To 021 demonstrate L A G O NN\u2019s value, we conduct a 022 thorough study of text classification systems in 023 the context of content moderation under four 024 label distributions. 1 025"
}