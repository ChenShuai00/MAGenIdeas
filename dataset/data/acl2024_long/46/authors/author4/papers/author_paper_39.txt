{
    "id": "8210cef990b8e5cddbc95000e46309bdd25337f7",
    "title": "Asking Clarification Questions for Code Generation in General-Purpose Programming Language",
    "abstract": "Code generation from text requires understanding the user\u2019s intent from a natural language description (NLD) and generating an executable program code snippet that satis\ufb01es this intent. While recent pretrained language models (PLMs) demonstrate remarkable performance for this task, these models fail when the given NLD is ambiguous due to the lack of enough speci\ufb01cations for generating a high-quality code snippet. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that ambiguities in the speci\ufb01cations of an NLD are resolved by asking clari\ufb01cation questions (CQs). Therefore, we collect and introduce a new dataset named CodeClarQA containing NLD-Code pairs with created CQAs. We evaluate the performance of PLMs for code generation on our dataset. The empirical results support our hypothesis that clari\ufb01cations result in more precise generated code, as shown by an improvement of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7% in the exact match. Alongside this, our task and dataset introduce new challenges to the community, including when and what CQs should be asked."
}