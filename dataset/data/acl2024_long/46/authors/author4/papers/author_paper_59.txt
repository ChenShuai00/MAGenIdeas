{
    "id": "0965e1ec743af08ce295653337ab4060df784d2a",
    "title": "Evidence-based Verification for Real World Information Needs",
    "abstract": "Claim veri\ufb01cation is the task of predicting the veracity of written statements against evidence. Previous large-scale datasets model the task as classi\ufb01cation, ignoring the need to retrieve evidence, or are constructed for research purposes, and may not be representative of real-world needs. In this paper, we introduce a novel claim veri\ufb01cation dataset with instances derived from search-engine queries, yielding 10,987 claims annotated with evidence that represent real-world information needs. For each claim, we annotate evidence from full Wikipedia articles with both section and sentence-level granularity. Our annotation allows comparison between two complementary approaches to veri\ufb01cation: stance classi-\ufb01cation, and evidence extraction followed by entailment recognition. In our comprehensive evaluation, we \ufb01nd no signi\ufb01cant difference in accuracy between these two approaches. This enables systems to use evidence extraction to summarize a rationale for an end-user while maintaining the accuracy when predicting a claim\u2019s veracity. With challenging claims and evidence documents containing hundreds of sentences, our dataset presents interesting challenges that are not captured in previous work \u2013 evidenced through transfer learning experiments. We release code and data 1 to support further research on this task."
}