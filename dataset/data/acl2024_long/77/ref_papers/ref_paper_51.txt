{
    "id": "bd58fc27b05f4d9a1e9b83d23afcd02bbc9ec491",
    "title": "DUE: End-to-End Document Understanding Benchmark",
    "abstract": "Understanding documents with rich layouts plays a vital role in digitization and hyper-automation but remains a challenging topic in the NLP research community. Additionally, the lack of a commonly accepted benchmark made it dif\ufb01cult to quantify progress in the domain. To empower research in this \ufb01eld, we intro-duce the Document Understanding Evaluation (DUE) benchmark consisting of both available and reformulated datasets to measure the end-to-end capabilities of systems in real-world scenarios. The benchmark includes Visual Question Answering, Key Information Extraction, and Machine Reading Comprehension tasks over various document domains and layouts featuring tables, graphs, lists, and infographics. In addition, the current study reports systematic baselines and analyzes challenges in currently available datasets using recent advances in layout-aware language modeling. We open both the benchmarks and reference imple-mentations and make them available at https://duebenchmark.com and https://github.com/due-benchmark ."
}