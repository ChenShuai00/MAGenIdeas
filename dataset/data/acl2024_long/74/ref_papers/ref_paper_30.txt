{
    "id": "3c61e6b55597cf37b19d2e4b38fc66b9c85c97b9",
    "title": "Ultra-Low Precision 4-bit Training of Deep Neural Networks",
    "abstract": "In this paper, we propose a number of novel techniques and numerical representation formats that enable, for the very \ufb01rst time, the precision of training systems to be aggressively scaled from 8-bits to 4-bits. To enable this advance, we explore a novel adaptive Gradient Scaling technique (GradScale) that addresses the challenges of insuf\ufb01cient range and resolution in quantized gradients as well as explores the impact of quantization errors observed during model training. We theoretically analyze the role of bias in gradient quantization and propose solutions that mitigate the impact of this bias on model convergence. Finally, we examine our techniques on a spectrum of deep learning models in computer vision, speech and NLP. In combination with previously proposed solutions for 4-bit quantization of weight and activation tensors, 4-bit training shows non-signi\ufb01cant loss in accuracy across application domains while enabling signi\ufb01cant hardware acceleration (>7 \u00d7 over state of the art FP16 systems)."
}