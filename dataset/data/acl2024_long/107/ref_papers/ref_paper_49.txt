{
    "id": "87d76615c649357d76c6adb5d317ced97851a0fc",
    "title": "Benchmark for Compositional Text-to-Image Synthesis",
    "abstract": "Rapid progress in text-to-image generation has been often measured by Fre\u00b4chet Inception Distance (FID) to capture how realistic the generated images are, or by R-Precision to assess if they are well conditioned on the given textual descriptions. However, a systematic study on how well the text-to-image synthesis models generalize to novel word compositions is missing. In this work, we focus on assessing how true the generated images are to the input texts in this particularly challenging scenario of novel compositions. We present the \ufb01rst systematic study of text-to-image generation on zero-shot compositional splits targeting two scenarios, unseen object-color ( e.g . \u201cblue petal\u201d) and object-shape ( e.g . \u201clong beak\u201d) phrases. We create new benchmarks building on the existing CUB and Oxford Flowers datasets. We also propose a new metric, based on a powerful vision-and-language CLIP model, which we leverage to compute R-Precision. This is in contrast to the common approach where the same retrieval model is used during training and evaluation, potentially leading to biased behavior. We experiment with several recent text-to-image generation methods. Our automatic and human evaluation con\ufb01rm that there is indeed a gap in performance when encountering previously unseen phrases. We show that the image correctness rather than purely perceptual quality is especially impacted. Finally, our CLIP-R-Precision metric demonstrates better correlation with human judgments than the commonly used metric. Dataset and evaluation code at: https://github.com/Seth-Park/comp-t2i-dataset ."
}