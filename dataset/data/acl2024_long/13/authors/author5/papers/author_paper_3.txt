{
    "id": "5d69c59e3a713e2e09a4f34ce2087d4279c56dac",
    "title": "SETE: Syntax-Enhanced Triplet Extraction with Semantic Consistency",
    "abstract": "Negation relationship is often absent from the predefined schemata of many Relational Triplet Extraction (RTE) tasks, resulting in the suboptimal performance of many previous RTE models on extracting negation relationships. To address this problem, we propose a novel Syntax-Enhanced Triplet Extraction (SETE) model in this paper, which can not only achieve ordinary RTE task, but also perform specially well on Negation Triplet Extraction (NTE) task. Achieving NET better is significant to many downstream applications, such as improving search/recommendation accuracy and personalization through filtering negative attributes of entities. For NTE task, we have further constructed a new dataset, NegComment, derived from real user review texts on Meituan. To improve extraction performance, our SETE model leverages syntactic information through incorporating the syntactic dependency tree into its Transformer architecture. Meanwhile, we introduce a novel semantic consistency auxiliary task to guide the generated triplets, so as to improve RTE performance further. Despite modifying the structure of existing Encoder-Decoder pretrained language models (PLMs), our model can be directly fine-tuned using pretrained checkpoints, thus leveraging the power of PLMs. Extensive experiments demonstrate that our model not only achieves the state-of-the-art (SOTA) performance on NTE, but also excels in other negation understanding tasks and generalizes well to ordinary RTE tasks."
}