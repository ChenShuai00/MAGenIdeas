{
    "id": "0d1775622abf795c9a6c8f55040fc134a9bb3c92",
    "title": "Joint Prediction of Punctuation and Disfluency in Speech Transcripts",
    "abstract": "Spoken language transcripts generated from Automatic speech recognition (ASR) often contain a large portion of dis\ufb02uency and lack punctuation symbols. Punctuation restoration and dis-\ufb02uency removal of the transcripts can facilitate downstream tasks such as machine translation, information extraction and syntactic analysis [1]. Various studies have shown the in\ufb02uence between these two tasks and thus performed modeling based on a multi-task learning (MTL) framework [2, 3], which learns general representations in the shared layers and separate representations in the task-speci\ufb01c layers. However, task dependencies are normally ignored in the task-speci\ufb01c layers. To model the dependencies of tasks, we propose an attention-based structure in the task-speci\ufb01c layers of the MTL framework incorporating the pretrained BERT (a state-of-art NLP-related model) [4]. Experimental results based on English IWSLT dataset and the Switchboard dataset show the proposed architecture outperforms the separate modeling methods as well as the traditional MTL methods."
}