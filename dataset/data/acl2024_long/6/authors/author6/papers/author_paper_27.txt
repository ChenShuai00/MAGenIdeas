{
    "id": "6a5c3dc4d827f4f5e41643277d6182256441c0f4",
    "title": "Efficient Video Representation Learning via Masked Video Modeling with Motion-centric Token Selection",
    "abstract": "Self-supervised Video Representation Learning (VRL) aims to learn transferrable representations from uncurated, unlabeled video streams that could be utilized for diverse downstream tasks. With recent advances in Masked Image Modeling (MIM), in which the model learns to predict randomly masked regions in the images given only the visible patches, MIM-based VRL methods have emerged and demonstrated their potential by signi\ufb01cantly outperforming previous VRL methods. However, they require an excessive amount of computations due to the added temporal dimen-sion. This is because existing MIM-based VRL methods overlook spatial and temporal inequality of information density among the patches in arriving videos by resorting to random masking strategies, thereby wasting computations on predicting uninformative tokens/frames. To tackle these limitations of Masked Video Modeling, we propose a new token selection method that masks our more important to-kens according to the object\u2019s motions in an online manner, which we refer to as Motion-centric Token Selection. Further, we present a dynamic frame selection strategy that allows the model to focus on informative and causal frames with minimal redundancy. We validate our method over multiple benchmark and Ego4D datasets, showing that the pre-trained model using our proposed method signi\ufb01cantly outperforms state-of-the-art VRL methods on downstream tasks, such as action recognition and object state change classi\ufb01cation while largely reducing memory requirements during pre-training and \ufb01ne-tuning."
}