{
    "id": "ccd06359dc314e5e66e87f0ec2b8921cdaed0a5c",
    "title": "Simple Weakly-Supervised Image Captioning via CLIP\u2019s Multimodal Embeddings",
    "abstract": "CLIP (Radford et al. 2021) enables strong performance in zero-shot image classification and other single-modality tasks through multi-modal pre-training. Recently, ClipCap (Mokady, Hertz, and Bermano 2021) demonstrated how the vision encoder of CLIP could be fed into GPT-2 to perform image captioning. In this work, we propose WS-ClipCap , which extends ClipCap to perform weakly-supervised image captioning by training only on the text from image captions. During training, WS-ClipCap encodes image captions using CLIP\u2019s text encoder. Then, during inference, WS-ClipCap encodes images using CLIP\u2019s vision encoder. Due to CLIP\u2019s joint embedding space for different modalities, the image and text representations are similar and can be interchanged. WS-ClipCap outperforms MAGIC (Su et al. 2022) substantially (which trains only on textual image captions) and performs on par with ESPER (Yu et al. 2022) (which trains only on images) while being significantly simpler than both. We also analyze how the performance of WS-ClipCap is affected by the distribution shift between CLIP\u2019s multi-modal embed-dings and investigate several ways of correcting the distribution mismatch."
}