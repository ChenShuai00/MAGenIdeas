{
    "id": "0914431aaa4ebf4b65316133aef6eedf1cccf10c",
    "title": "Supplementary for Paper: Improving Vision-and-Language Navigation by Generating Future-View Image Semantics",
    "abstract": "HAMT [1] utilizes a transformer-based architecture to encode the instructions, navigation history, and current step observation. Specifically, the instructions are encoded with a BERT architecture. As the navigation history is a sequence of panorama observations, HAMT encodes the navigation history with a hierarchical architecture. It first uses a panorama encoder to encode panorama into view representation, and uses multiple transformer layers as the temporal encoder to encode the observations on the trajectory. Formally, given the encoded history observation vi, which is the output of the panorama encoder, the output of the temporal encoder is hi = LN(Wtvi) + LN(Waai) + E S i + E T 2 , where ai is the action embedding at step i, E i is the step encoding, and E 2 is the token type encoding which indicates the input is history views. The current step observation is represented as 36 discretized views. Each view is passed through the transformer encoder to learn the view representation: oi = LN(Wov o i ) + LN(W o aa o i ) + E O i + E T 1 , where v o i is the encoding of view i, ai is the action embedding for view i, E i is the embedding indicating whether the current view is navigable, and E 1 is the token type encoding which indicates the input is current step observation. The agent predicts the next step by comparing the similarity between the observation encoding oi and the <CLS> token which contains instruction-trajectory information. More implementation details can be found in [1]."
}