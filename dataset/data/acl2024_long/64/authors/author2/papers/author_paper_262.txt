{
    "id": "f212791a9d2f5bcc155e619dc6d7f13ed59f2558",
    "title": "Research Statement Mohit Bansal",
    "abstract": "For artificial intelligent agents to function autonomously in our homes and workplaces, they must be able to effectively understand natural language for instructions and communication. For this, it is necessary to resolve the various deep and subtle semantic (meaning-related) ambiguities in natural language, a challenging goal that involves two fundamental requirements. First, we need diverse, external world knowledge which is simply not present in the standard training datasets used for supervised NLP tasks. Second, we need to develop appropriate machine learning models that can extract the precise disambiguation cues lying latent in such diverse, large-scale data. My research addresses both these requirements by learning novel weakly-labeled and cross-modal semantic representations with accurate, well-formulated disambiguation models, achieving the state-of-the-art on various core NLP tasks and multimodal applications. I first model world knowledge by automatically extracting pattern statistics from a huge unlabeled corpus of Web text. When augmented with these robust surface statistics, my supervised structured methods do a better job at resolving sentenceand document-level ambiguities, and at learning various types of semantic relations. In addition to pattern-based knowledge, language understanding can also be improved via dense (continuous space), distributed representations of language. These unsupervised embeddings, usually trained on large amounts of unlabeled surrounding context, capture useful, finegrained meaning relationships but with a generic similarity that mixes concepts such as antonyms and hypernyms. My research learns stronger, relation-discriminative embedding spaces via weak supervision from automatically extracted knowledge bases and improved neural models, achieving strong improvements in various NLP tasks, with human-level performance in some cases. Grounding cues (i.e., mapping words to physical things in the world) from other modalities such as vision and speech can help resolve tricky pragmatic (contextual) ambiguities which cannot be resolved using any amount of textual data. With colleagues, I show how coreference ambiguities in multisentence image captions can be resolved using visual, alignment-based cues in a multimodal structured prediction formulation. I also develop a neural translation style method to successfully understand and execute navigational instructions by using visual cues from an environment map. Moreover, subtle syntactic ambiguities in spoken language can also be resolved using prosodic cues from speech signals. My research develops diverse learning methods to accurately model the underlying semantics for the task at hand, e.g., from probabilistic graphical models for structured prediction in taxonomy induction and image-text coreference to recursive and recurrent neural networks (with varying loss functions) for embedding learning and sequence-to-sequence tasks, to a combination of structure and neural methods (e.g., coarse-to-fine alignment in encoder-decoder models). A common theme in my model formulations is to employ multiple \u2018views,\u2019 e.g., two languages or a paraphrase pair, image and text, navigational instruction and actions in a map image, speech and text, database and language, etc."
}