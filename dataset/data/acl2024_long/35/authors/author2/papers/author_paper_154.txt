{
    "id": "7a0ffb5493be9f82c5c71cb4b288acc318d4df06",
    "title": "Universal Speech Transformer",
    "abstract": "Transformer model has made great progress in speech recognition. However, compared with models with iterative computation, transformer model has \ufb01xed encoder and decoder depth, thus losing the recurrent inductive bias. Besides, \ufb01nding the optimal number of layers involves trial-and-error attempts. In this paper, the universal speech transformer is proposed, which to the best of our knowledge, is the \ufb01rst work to use universal transformer for speech recognition. It generalizes the speech transformer with dynamic numbers of encoder/decoder layers, which can relieve the burden of tuning depth related hyperpa-rameters. Universal transformer adds the depth and positional embeddings repeatedly for each layer, which dilutes the acoustic information carried by hidden representation, and it also performs a partial update of hidden vectors between layers, which is less ef\ufb01cient especially on the very deep models. For better use of universal transformer, we modify its processing framework by removing the depth embedding and only adding the positional embedding once at transformer encoder frontend. Furthermore, to update the hidden vectors ef\ufb01ciently, especially on the very deep models, we adopt a full update. Experiments on LibriSpeech, Switchboard and AISHELL-1 datasets show that our model outperforms a baseline by 3.88%-13.7%, and surpasses other model with less computation cost."
}