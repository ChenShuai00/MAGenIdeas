{
    "id": "14cc6b84819c67cd3fe6131054dd54aeae516e67",
    "title": "Simultaneous Machine Translation with Visual Context",
    "abstract": "Simultaneous Machine Translation (SiMT) is a subfield of Natural Language Processing that focuses on translating continuous streams of sentences from one language to another with the lowest latency and highest quality possible. Previous works and empirical research have shown that having supplementary information can improve the performance of a language model. The main hypothesis for this project was that having supplementary information from an image will make up for the missing textual information. Language barriers can have multiple implications in fields like healthcare, education, aviation and multilateral negotiations which is why the development of such a system is required. The project has been using the Multi30K dataset and pysimt library for training and evaluation. The first phase of the project was completed in January and an improvement of roughly 2 BLEU points was been achieved with the help of visual context. The next phase of the project looked into several methods that can be used to improve the performance of the SiMT system. On analyzing the Multi30K dataset, other online available datasets as well as recent publications, I came to the conclusion that creating a new dataset for the task of Simultaneous Machine Translation was the best option. Therefore, a new dataset was created by scraping the video from TED Talks. In the end, the dataset contains 4024 videos with subtitles available in several different languages. The dataset is roughly 118GB in size and using a segmentation algorithm, this model can be run on the current SiMT model. The main hypothesis for this part is that TED Talks that contain slides, animations or other resources for explaining the content will provide valuable information in the form of visual features that can be extracted using a ResNet CNN which will improve the performance of the simultaneous machine translation system."
}