{
    "id": "4554804ab235a459ac28b37b7c157816afe5b9b4",
    "title": "Content-Adaptive Pixel Discretization to Improve Model Robustness",
    "abstract": "Preprocessing defenses such as pixel discretization are appealing to remove adversarial attacks due to their simplicity. However, they have been shown to be ine\ufb00ective except on simple datasets like MNIST. We hypothesize that existing discretization approaches failed because using a \ufb01xed codebook for the entire dataset limits their ability to balance image representation and codeword separability. We \ufb01rst formally prove that adaptive codebooks can provide stronger robustness guarantees than \ufb01xed codebooks as a preprocessing defense on some datasets. Based on that insight, we propose a content-adaptive pixel discretization defense called Essential Features , which discretizes the image to a per-image adaptive codebook to reduce the color space. We then \ufb01nd that Essential Features can be further optimized by applying adaptive blurring before the discretization to push perturbed pixel values back to their original value before determining the codebook. Against adaptive attacks, we show that content-adaptive pixel discretization extends the range of datasets that bene\ufb01t in terms of both L 2 and L \u221e robustness where previously \ufb01xed codebooks were found to have failed. Our \ufb01ndings suggest that content-adaptive pixel discretization should be part of the repertoire for making models robust. clear perturbations but a similar SSIM score as a pretrained ATTA baseline attack. This shows that neither SSIM nor L \u221e necessarily capture the true increase of perceptibility. We leave more exploration to future work."
}