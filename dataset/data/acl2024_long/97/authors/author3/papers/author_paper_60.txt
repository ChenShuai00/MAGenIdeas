{
    "id": "04713df8c1dd240188375f1814df6e48d7689295",
    "title": "Wearable Technology Brings Security to Alexa and Siri",
    "abstract": "IT companies are heavily investing in the future of voice assistants. Siri, Cortana, Google Now, Alexa, and Samsung Bixby are already part of our everyday fixtures. Through voice interactions, voice assistants allow us to place phone calls, send messages, check emails, schedule appointments, navigate to destinations, control smart appliances, and perform banking services. In numerous scenarios, such as cooking, exercising or driving, voice interaction is preferable to traditional touch interfaces that are inconvenient or even dangerous to use. Further, a voice interface is even more essential for the increasingly prevalent Internet of Things (IoT) devices that lack touch capabilities. Security concerns, however, have become a major roadblock against the adoption of voice-based interactions. With sound being an open channel, voice as an input mechanism is inherently insecure as it is prone to replay attacks, sensitive to noise, and easy to impersonate. Recent studies have even demonstrated that it is possible to inject voice commands stealthily and remotely with mangled voice [1, 2], ultrasound [12], wireless signals [3], or through public radio stations [4] without the user\u2019s awareness. Existing voice authentication mechanisms, such as Google\u2019s \u201cTrusted Voice\u201d and Nuance\u2019s \u201cFreeSpeech\u201d (used by banks [5]) fail to provide the security necessary for voice-assistant systems. An attacker can bypass these voice-as-biometric authentication mechanisms by impersonating the user\u2019s voice (a feature already enabled by commercial software, such as Adobe Voco) or simply by recording and replaying the user\u2019s voice. Even Google warns against its voice authentication feature as being insecure, and some security companies [6] recommend Huan Feng Facebook Inc Kassem Fawaz University of Wisconsin-Madison Kang G. Shin University of Michigan"
}