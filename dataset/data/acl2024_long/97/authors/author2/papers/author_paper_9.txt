{
    "id": "71d5d8ec7a42a33ad6ae319c13d9ee9b519fd7b4",
    "title": "Computational Hardness in Robust Learning",
    "abstract": "Over recent years, learning classi\ufb01ers that are robust to adversarial perturbations has emerged as a crucial, yet challenging problem. There are different approaches to reduce the classi\ufb01cation error over adversarial examples, but their success is still limited. To understand this phenomenon, a line of works have provided evidence that adversarial vulnerability is inherently due to the computational limitations of the tasks or learning algorithms. In this survey, inspired by the work of Bubeck, Lee, Price and Razenshteyn, we investigate examples of classi\ufb01cation tasks where (1) no robust classi\ufb01er is possible, (2) only inef\ufb01cient robust classi\ufb01er is possible, (3) learning robust classi\ufb01ers requires large sample complexity, or (4) an ef\ufb01cient robust classi\ufb01er exists, but no ef\ufb01cient learning algorithm is possible. We hope that this survey will encourage the research on developing these ideas to the problems in modern machine learning that involves neural networks, or on studying new learning algorithms for computationally ef\ufb01cient robust classi\ufb01cations."
}