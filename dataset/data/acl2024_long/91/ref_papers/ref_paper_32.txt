{
    "id": "ac4ffaab10f6b6ad83e79ca5691f338abf5cff82",
    "title": "Instruction Mining: High-Quality Instruction Data Selection for Large Language Models",
    "abstract": "Large language models typically undergo two training stages, pretraining and \ufb01netuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models\u2019 ability of interpreting and responding to instructions, instruction \ufb01netuning has emerged as a critical method in this area. Recent studies found that large language models can be \ufb01netuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for \ufb01netuning language models still lacks clear guidelines to follow. In this paper, we propose I NSTRUCT M INING , a linear rule for evaluating instruction-following data quality. We formulate I NSTRUCT M INING using speci\ufb01c natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive \ufb01netuning experiments. The experiment results are then applied to estimating parameters in I NSTRUCT M INING . To further investigate its performance, we use I NSTRUCT M INING to select high-quality data from unseen datasets. Results demonstrate that I NSTRUCT M INING can help select relatively high-quality samples from various instruction-following datasets. Compared to models \ufb01netuned on un\ufb01ltered datasets, models \ufb01netuned on I NSTRUCT M INING selected datasets perform better on 42.5% cases."
}