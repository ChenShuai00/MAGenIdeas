{
    "id": "c8c25de4164158c44b39656d6a4406c8ee1b615b",
    "title": "Multi-Narrative Semantic Intersection Task: Evaluation and Benchmark",
    "abstract": "In this paper, we introduce an impor-001 tant yet relatively unexplored NLP task 002 called Multi-Narrative Semantic Intersection 003 (MNSI), which entails generating a Seman-004 tic Intersection of multiple alternate narratives. 005 As no benchmark dataset is readily available 006 for this task, we created one by crawling 2 , 925 007 alternative narrative pairs from the web and 008 then, went through the tedious process of man-009 ually creating 411 different ground-truth se-010 mantic intersections by engaging human anno-011 tators. As a way to evaluate this novel task, 012 we \ufb01rst conducted a systematic study by bor-013 rowing the popular ROUGE metric from text-014 summarization literature and discovered that 015 ROUGE is not suitable for our task. Subse-016 quently, we conducted further human annota-017 tions/validations to create 200 document-level 018 and 1 , 518 sentence-level ground-truth labels 019 which helped us formulate a new precision-020 recall style evaluation metric, called SEM-021 F1 (semantic F1), based on presence, partial-022 presence and absence of information. Exper-023 imental results show that the proposed SEM-024 F1 metric yields higher correlation with hu-025 man judgement as well as higher inter-rater-026 agreement compared to ROUGE metric and 027 thus, we recommend the community to use this 028 metric for evaluating future research on this 029 topic. 030"
}