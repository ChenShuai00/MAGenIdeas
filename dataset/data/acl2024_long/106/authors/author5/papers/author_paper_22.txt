{
    "id": "a75ce42f9b9d6d90eada8df724df5b32d6fd4443",
    "title": "TACO: Pre-training of Deep Transformers with Attention Convolution using Disentangled Positional Representation",
    "abstract": "Word order, as a crucial part to understand 001 natural language, has been carefully consid-002 ered in pre-trained models by incorporating 003 different kinds of positional encodings. How-004 ever, existing pre-trained models mostly lack 005 the ability to maintain robustness against mi-006 nor permutation of words in learned repre-007 sentations. We therefore propose a novel ar-008 chitecture named T ransformer with A ttention 009 CO nvolution ( TACO ), to explicitly disentan-010 gle positional representations and incorporate 011 convolution over multi-source attention maps 012 before softmax in self-attention. Additionally, 013 we design a novel self-supervised task, masked 014 position modeling (MPM), to assist our TACO 015 model in capturing complex patterns with re-016 gard to word order. Combining MLM (masked 017 language modeling) and MPM objectives, the 018 proposed TACO model can efficiently learn 019 two disentangled vectors for each token, rep-020 resenting its content and position respectively. 021 Experimental results show that TACO signif-022 icantly outperforms BERT in various down-023 stream tasks with fewer model parameters. Re-024 markably, TACO achieves +2.6% improvement 025 over BERT on SQuAD 1.1 task, +5.4% on 026 SQuAD 2.0 and +3.4% on RACE, with only 027 46K pre-training steps. 028"
}