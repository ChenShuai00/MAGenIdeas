{
    "id": "c5a21b7db5ecd5832765b66f48e034debc6b72b7",
    "title": "Towards Fully Self-Supervised Knowledge Learning from Unstructured Text",
    "abstract": "Pre-trained language models (PLMs) like 001 BERT have made signi\ufb01cant progress in var-002 ious downstream NLP tasks. However, by ask-003 ing models to do cloze-style tests, recent work 004 \ufb01nds that PLMs are short in acquiring knowl-005 edge from the unstructured text. To understand 006 the internal behavior of PLMs in retrieving 007 knowledge, we \ufb01rst de\ufb01ne knowledge-baring 008 (K-B) tokens and knowledge-free (K-F) tokens 009 for unstructured text and ask professional an-010 notators to manually label some sample. Then, 011 we \ufb01nd that PLMs are more likely to give 012 wrong predictions on K-B tokens and attend 013 less attention to those tokens inside the self-014 attention module. Based on these observations, 015 we develop two solutions to help the model 016 learn more knowledge from the unstructured 017 text in a fully self-supervised manner. Exper-018 iments on knowledge probing tasks show the 019 effectiveness of the proposed methods. To our 020 best knowledge, we are the \ufb01rst to explore 021 fully self-supervised learning of knowledge in 022 continue pre-training. 1 023"
}