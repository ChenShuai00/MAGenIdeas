{
    "Title": "Interdisciplinary Faithfulness: Integrating Cognitive Science and AI to Measure Explanation Quality",
    "Idea": "This idea proposes an interdisciplinary approach to measuring the faithfulness of LLM-generated explanations by integrating insights from cognitive science and AI. The framework leverages cognitive theories of human reasoning and explanation generation to design new metrics and tests for evaluating the quality of explanations. Specifically, it uses cognitive models of how humans generate and evaluate explanations to identify key features of faithful explanations, such as coherence, relevance, and causal consistency. These features are then operationalized into quantitative metrics that can be applied to LLM-generated explanations. The framework also includes a human-in-the-loop evaluation component, where human judges assess the quality of explanations based on cognitive principles.",
    "Thinking": "This idea is inspired by **Kuhn’s paradigm theory** and **Laudan’s methodological improvement model**. Kuhn’s paradigm theory emphasizes the importance of integrating interdisciplinary knowledge to discover new problems and improve existing theories. By combining insights from cognitive science and AI, this framework addresses the limitations of current faithfulness tests and provides a more holistic approach to evaluating explanations. Laudan’s methodological improvement model focuses on improving existing methods by integrating new technologies and tools, which aligns with the goal of enhancing the evaluation of LLM-generated explanations through interdisciplinary collaboration.",
    "Rationale": "Current methods for evaluating the faithfulness of LLM-generated explanations are often limited to surface-level metrics and lack a theoretical foundation in human reasoning. This interdisciplinary framework addresses this gap by leveraging cognitive science to design more robust and interpretable metrics. The integration of human-in-the-loop evaluation also ensures that the metrics align with human intuitions about explanation quality. This idea has the potential to significantly advance the field of explainable AI by providing a more rigorous and human-centered approach to evaluating explanations, making it a strong candidate for best paper awards at top conferences."
}