{
    "id": "6c5860bb9b2e2469e6cf9deb0ba2baf460871067",
    "title": "Controlling Global Statistics in Recurrent Neural Network Text Generation",
    "abstract": "\n \n Recurrent neural network language models (RNNLMs) are an essential component for many language generation tasks such as machine translation, summarization, and automated conversation. Often, we would like to subject the text generated by the RNNLM to constraints, in order to overcome systemic errors (e.g. word repetition) or achieve application-specific goals (e.g. more positive sentiment). In this paper, we present a method for training RNNLMs to simultaneously optimize likelihood and follow a given set of statistical constraints on text generation.\u00a0 The problem is challenging because the statistical constraints are defined over aggregate model behavior, rather than model parameters, meaning that a straightforward parameter regularization approach is insufficient.\u00a0 We solve this problem using a dynamic regularizer that updates as training proceeds, based on the generative behavior of the RNNLMs.\u00a0 Our experiments show that the dynamic regularizer outperforms both generic training and a static regularization baseline.\u00a0 The approach is successful at improving word-level repetition statistics by a factor of four in RNNLMs on a definition modeling task.\u00a0 It also improves model perplexity when the statistical constraints are $n$-gram statistics taken from a large corpus.\n \n"
}