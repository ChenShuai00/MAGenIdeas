{
    "id": "024908a72e2a7eeb59cc4183c028de19207cac40",
    "title": "The State of Intent Detection in the Era of Large Autoregressive Language Models",
    "abstract": "In-context learning (ICL) using large pre-001 trained autoregressive language models (LLMs, 002 e.g. GPT-3) has demonstrated effective clas-003 sification performance at a variety of natural 004 language tasks. Using LLMs for intent detec-005 tion is challenging due to the large label space 006 and limited context window, such that it is diffi-007 cult to fit a sufficient number of examples in the 008 prompt to allow the use of in-context learning. 009 In this paper, dense retrieval is used to bypass 010 this limitation, giving the model only a par-011 tial view of the full label space. We show that 012 retriever-augmented large language models are 013 an effective way to tackle intent detection, by-014 passing context window limitations effectively 015 through the retrieval mechanism. Comparing 016 the LLaMA and OPT model families at differ-017 ent scales, we set new state of the art perfor-018 mance in the few-shot setting with zero training 019 for two of the three intent classification datasets 020 that we consider, while achieving competitive 021 results on the third one. This work demon-022 strates that the Retriever+ICL framework is a 023 strong zero-training competitor to fine-tuned in-024 tent detection approaches. In addition, a small 025 study on the number of examples provided at 026 different model scales is done, showing that 027 larger models are needed to make effective use 028 of more examples in-prompt. 029"
}