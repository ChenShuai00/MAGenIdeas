{
    "id": "2065bd4e27d878c944542db45c439ee53a2e7b24",
    "title": "Dimensionality Reduction in Semantic Vector Spaces Using a Derivational Resource",
    "abstract": "Lexical semantic vector spaces model the meaning of words by representing the cooccurrence statistics of words in certain contexts. Words are considered similar if they occur in similar contexts, so that word similarity can get predicted by comparing their representation in the semantic vector space. Two major problems in those vector spaces are their size and their sparsity. Due to the characteristics of language, semantic vector space models built from large corpora tend to grow immensely in dimensionality and therefore in size. They capture co-occurrence information in a sparse way. On the one hand sparse co-occurrence matrices can be represented and stored more efficiently than dense, completely filled matrices, but on the other hand they limit the vector space\u2019s ability to make semantic predictions. Dimensionality reduction techniques aim to solve this problem by reducing the vector spaces\u2019 number of dimensions. With this thesis, we introduce a linguistically motivated approach for dimensionality reduction: We abstract from the meaning of single words to the meaning of derivational families building on DErivBase [Zeller et al., 2013], a resource for German which groups words into derivational families. DErivBase allows us to partition the original dimensions into equivalence classes corresponding to derivational families. The result of this transformation is a dimensionality-reduced vector space with derivational families representing the dimensions. In two experiments (looking at word-based and dependency-based semantic vector spaces, respectively), we evaluate on two benchmark data sets for two standard tasks, namely semantic relatedness scoring and synonym detection, with a varying range of transformation parameters. We analyse the following aspects: dimensionality and sparsity reduction, size, and quality of semantic relatedness predictions. We compare our approach against competitors and baselines, including singular value decomposition (SVD) and a dimension selection approach (top-n). We find that our novel technique brings about a substantial reduction of dimensions, and allows highly compact representation and storage of the vector space model. It performs better on word-based than on dependency-based vector space models, and better on semantic relatedness than on synonym choice tasks. It is superior to SVD in"
}