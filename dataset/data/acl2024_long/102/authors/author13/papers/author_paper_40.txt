{
    "id": "6adc9c231d874ea358554b8680a6aaba4bd6c963",
    "title": "MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer",
    "abstract": "Adapter modules have emerged as a general parameter-ef\ufb01cient means to specialize a pre-trained encoder to new domains. Massively multilingual transformers (MMTs) have particularly bene\ufb01ted from additional training of language-speci\ufb01c adapters. However, this approach is not viable for the vast majority of languages, due to limitations in their corpus size or compute budgets. In this work, we propose MAD-G ( M ultilingual AD apter G eneration), which contextually generates language adapters from language representations based on typological features. In contrast to prior work, our time-and space-ef\ufb01cient MAD-G approach enables (1) sharing of linguistic knowledge across languages and (2) zero-shot inference by generating language adapters for unseen languages. We thoroughly evaluate MAD-G in zero-shot cross-lingual transfer on part-of-speech tagging, dependency parsing, and named entity recognition. While offering (1) improved \ufb01ne-tuning ef\ufb01ciency (by a factor of around 50 in our experiments), (2) a smaller parameter budget, and (3) increased language coverage, MAD-G remains competitive with more expensive methods for language-speci\ufb01c adapter training across the board. Moreover, it offers substantial bene\ufb01ts for low-resource languages, particularly on the NER task in low-resource African languages. Finally, we demonstrate that MAD-G\u2019s transfer performance can be further improved via: (i) multi-source training , i.e., by generating and combining adapters of multiple languages with available task-speci\ufb01c training data; and (ii) by further \ufb01ne-tuning generated MAD-G adapters for languages with monolingual data."
}