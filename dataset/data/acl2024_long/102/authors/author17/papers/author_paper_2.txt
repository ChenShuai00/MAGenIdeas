{
    "id": "bccd50c0648be7937847ff7573756478cd0604ce",
    "title": "Think Twice: Measuring the Efficiency of Eliminating Model Biases in Question Answering",
    "abstract": "While the Large Language Models (LLMs) 001 dominate a majority of language understand-002 ing tasks, previous work shows that some of 003 these results are supported by modeling biases 004 of training datasets. Authors commonly assess 005 model robustness by evaluating their models on 006 out-of-distribution (OOD) datasets of the same 007 task, but these datasets might share the biases 008 of the training dataset. 009 We introduce a framework for finer-grained 010 analysis of discovered model biases and mea-011 sure the significance of some previously-012 reported biases while uncovering several new 013 ones. The bias-level metric allows us to as-014 sess how well different pre-trained models and 015 state-of-the-art debiasing methods mitigate the 016 identified biases in Question Answering (QA) 017 and compare their results to a resampling base-018 line. We find cases where bias mitigation hurts 019 OOD performance and, on the contrary, when 020 bias enlargement corresponds to improvements 021 in OOD, suggesting that some biases are shared 022 among QA datasets and motivating future work 023 to refine the analyses of LLMs\u2019 robustness. 024"
}