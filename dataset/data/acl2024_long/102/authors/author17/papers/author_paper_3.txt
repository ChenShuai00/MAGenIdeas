{
    "id": "6dd8784b8e2782fc28b5dd0b1073d60069fa991d",
    "title": "What Makes Machine Reading Comprehension Questions Dif\ufb01cult? Investigating Variation in Passage Sources and Question Types",
    "abstract": "For a natural language understanding bench-001 mark to be useful in research, it has to con-002 sist of examples that are diverse and dif\ufb01-003 cult enough to discriminate among current and 004 near-future state-of-the-art systems. However, 005 we do not yet know how best to select pas-006 sages to collect a variety of challenging exam-007 ples. In this study, we crowdsource multiple-008 choice reading comprehension questions for 009 passages taken from seven qualitatively dis-010 tinct sources, analyzing what attributes of pas-011 sages contribute to the dif\ufb01culty and question 012 types of the collected examples. To our sur-013 prise, we \ufb01nd that passage source, length, and 014 readability measures do not signi\ufb01cantly affect 015 question dif\ufb01culty. Through our manual anno-016 tation of seven reasoning types, we observe 017 several trends between passage sources and 018 reasoning types, e.g., logical reasoning is more 019 often required in questions written for techni-020 cal passages. These results suggest that when 021 creating a new benchmark dataset, selecting a 022 diverse set of passages can help ensure a di-023 verse range of question types, but that passage 024 dif\ufb01culty need not be a priority. 025"
}