{
    "id": "e512964293671abbdc409f313d127cbe85ffe5cd",
    "title": "GENIE : Large Scale Pre-training for Generation with Diffusion Model",
    "abstract": "In this paper, we propose a large-scale language pre-training for text GEN eration using d I ffusion mod E l, which is named GENIE . GENIE is a pre-training sequence-to-sequence text generation model which combines Transformer and diffusion. The diffusion model accepts the latent information from the encoder, which is used to guide the denoising of the current time step. After multiple such denoise iterations, the diffusion model can restore the Gaussian noise to the diverse output text which is controlled by the input text. More-over, such architecture design also allows us to adopt large scale pre-training on the GENIE . We propose a novel pre-training method named continuous paragraph denoise based on the characteristics of the diffusion model. Extensive experiments on the XS UM , CNN/D AILY M AIL , and G IGAWORD benchmarks shows that GENIE can achieves comparable performance with various strong baselines, especially after pre-training, the generation quality of GENIE is greatly improved. We have also conduct a lot of experiments on the generation diversity and parameter impact of GENIE . The code for GENIE will be made publicly available."
}