{
    "id": "db20bd3bb82d1011ce704d440d8c2578f665e6e1",
    "title": "Aligning Robot and Human Representations",
    "abstract": "\u2014To act in the world, robots rely on a representation of salient task aspects: for example, to carry a cup of coffee, a robot must consider movement ef\ufb01ciency and cup orientation in its behaviour. However, if we want robots to act for and with people , their representations must not be just functional but also re\ufb02ec-tive of what humans care about, i.e. their representations must be aligned with humans\u2019. In this survey, we pose that current reward and imitation learning approaches suffer from representation misalignment , where the robot\u2019s learned representation does not capture the human\u2019s true representation. We suggest that because humans will be the ultimate evaluator of robot performance in the world, it is critical that we explicitly focus our efforts on aligning learned task representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. To do so, we mathematically de\ufb01ne the problem, identify its key desiderata, and situate current robot learning methods within this formalism. We conclude the survey by suggesting future directions for exploring open challenges."
}