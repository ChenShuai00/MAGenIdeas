{
    "id": "92bcd3760fc07f19922496c87ea3912917a2a5e5",
    "title": "CodeBERT\u2010Attack: Adversarial attack against source code deep learning models via pre\u2010trained model",
    "abstract": "Over the past few years, the software engineering (SE) community has widely employed deep learning (DL) techniques in many source code processing tasks. Similar to other domains like computer vision and natural language processing (NLP), the state\u2010of\u2010the\u2010art DL techniques for source code processing can still suffer from adversarial vulnerability, where minor code perturbations can mislead a DL model's inference. Efficiently detecting such vulnerability to expose the risks at an early stage is an essential step and of great importance for further enhancement. This paper proposes a novel black\u2010box effective and high\u2010quality adversarial attack method, namely CodeBERT\u2010Attack (CBA), based on the powerful large pre\u2010trained model (i.e., CodeBERT) for DL models of source code processing. CBA locates the vulnerable positions through masking and leverages the power of CodeBERT to generate textual preserving perturbations. We turn CodeBERT against DL models and further fine\u2010tuned CodeBERT models for specific downstream tasks, and successfully mislead these victim models to erroneous outputs. In addition, taking the power of CodeBERT, CBA is capable of effectively generating adversarial examples that are less perceptible to programmers. Our in\u2010depth evaluation on two typical source code classification tasks (i.e., functionality classification and code clone detection) against the most widely adopted LSTM and the powerful fine\u2010tuned CodeBERT models demonstrate the advantages of our proposed technique in terms of both effectiveness and efficiency. Furthermore, our results also show (1) that pre\u2010training may help CodeBERT gain resilience against perturbations further, and (2) certain pre\u2010training tasks may be beneficial for adversarial robustness."
}