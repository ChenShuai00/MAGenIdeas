{
    "id": "4e7cdd83f844044c2661204602bac108e2ccf600",
    "title": "IRCoCo: Immediate Rewards-Guided Deep Reinforcement Learning for Code Completion",
    "abstract": "Code completion aims to enhance programming productivity by predicting potential code based on the current \n \nprogramming context. Recently, pre-trained language models (LMs) have become prominent in this field. \n \nVarious approaches have been proposed to fine-tune LMs using supervised fine-tuning (SFT) techniques for \n \ncode completion. However, the inherent exposure bias of these models can cause errors to accumulate early in \n \nthe sequence completion, leading to even more errors in subsequent completions. To address this problem, \n \ndeep reinforcement learning (DRL) is an alternative technique for fine-tuning LMs for code completion, which \n \ncan improve the generalization capabilities and overall performance. Nevertheless, integrating DRL-based \n \nstrategies into code completion faces two major challenges: 1) The dynamic nature of the code context requires \n \nthe completion model to quickly adapt to changes, which poses difficulties for conventional DRL strategies \n \nthat focus on delayed rewarding of the final code state. 2) It is difficult to evaluate the correctness of partial \n \ncode, thus the reward redistribution-based strategies cannot be adapted to code completion. To tackle these \n \nchallenges, we propose IRCoCo, a code completion-specific DRL-based fine-tuning framework. This framework \n \nis designed to provide immediate rewards as feedback for detecting dynamic context changes arising from \n \ncontinuous edits during code completion. With the aid of immediate feedback, the fine-tuned LM can gain \n \na more precise understanding of the current context, thereby enabling effective adjustment of the LM and \n \noptimizing code completion in a more refined manner. Experimental results demonstrate that fine-tuning pre-trained LMs with IRCoCo leads to significant improvements in the code completion task, outperforming \n \nboth SFT-based and other DRL-based baselines."
}