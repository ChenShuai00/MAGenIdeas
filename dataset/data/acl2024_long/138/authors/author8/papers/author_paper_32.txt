{
    "id": "301d980c707b2152fc350a04b877f19bfe1b0f87",
    "title": "A Survey in Adversarial Defences and Robustness in NLP",
    "abstract": "In recent years, it has been seen that deep neural networks are lacking robustness and are likely to break in case of adversarial perturbations in input data. Strong adversarial attacks are proposed by various authors for computer vision and Natural Language Processing (NLP). As a counter-e\ufb00ort, several defense mechanisms are also proposed to save these networks from failing. In contrast with image data, generating adversarial attacks and defending these models is not easy in NLP because of the discrete nature of the text data. However, numerous methods for adversarial defense are proposed of late, for di\ufb00erent NLP tasks such as text classi\ufb01cation, named entity recognition, natural language inferencing, etc. These methods are not just used for defending neural networks from adversarial attacks, but also used as a regularization mechanism during training, saving the model from over\ufb01tting. The proposed survey is an attempt to review di\ufb00erent methods proposed for adversarial defenses in NLP in the recent past by proposing a novel taxonomy. This survey also highlights the fragility of the advanced deep neural networks in NLP and the challenges in defending them."
}