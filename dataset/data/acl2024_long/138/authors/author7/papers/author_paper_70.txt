{
    "id": "6dbbade359d2830cf86691fd188e285c7571b41c",
    "title": "NICT-5\u2019s Submission To WAT 2021: MBART Pre-training And In-Domain Fine Tuning For Indic Languages",
    "abstract": "In this paper we describe our submission to the multilingual Indic language translation wtask \u201cMultiIndicMT\u201d under the team name \u201cNICT-5\u201d. This task involves translation from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training."
}