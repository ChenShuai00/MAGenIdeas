{
    "id": "ee4da43bc8096832ce9d0084e316e9952d1c74bc",
    "title": "Multilingual and Multi-Domain Adaptation for Neural Machine Translation",
    "abstract": "Neural machine translation (NMT) (Bahdanau et al., 2015) allows one to train an end-to-end system without the need to deal with word alignments, translation rules and complicated decoding algorithms, which are a characteristic of statistical machine translation (SMT) systems. A number of studies have shown that vanilla NMT works better than SMT only when there is an abundance of parallel corpora (Zoph et al., 2016). In a low resource situation, it is important to apply various domain adaptation techniques for NMT to beat SMT (Chu et al., 2017). Domain adaptation is the process of developing high quality domain specific NLP models by leveraging out-of-domain data or models in order to improve the in-domain performance. In the context of NMT, several domain adaptation approaches have been proposed and shown to be effective in a low resource scenario (Chu et al., 2017). Most domain adaptation approaches focus on using a single resource rich out-of-domain data source to improve the low resource in-domain translations. There are also studies that use multiple out-ofdomain data for adaptation (Sajjad et al., 2017). It may not always be possible to use an out-ofdomain parallel corpus in the same language pair and thus it is important to use data from other languages (Johnson et al., 2016). This approach is known as cross-lingual transfer learning, which transfers NMT model parameters among multiple languages. It is well known that a multilingual model, which relies on parameter sharing, helps in improving the translation quality for low resource languages especially when the target language is the same (Zoph et al., 2016). In this paper, we propose to simultaneously use both, multilingual and multi-domain data for domain adaptation of NMT, which might outperform the methods that use them independently. To the best of our knowledge, this is the first study that uses both multilingual and multi-domain data for domain adaptation. To verify the effective methods in this multilingual and multi-domain adaptation scenario, we compare the different methods in the empirical study of single language pair domain adaptation for NMT (Chu et al., 2017). In particular, we compare fine tuning, multi-domain and mixed fine tuning (Chu et al., 2017). We study how multilingualism impacts the indomain translation performance and how transfer learning can be performed by fine tuning multilingual out-of-domain models to learn multilingual in-domain models."
}