{
    "id": "3aa443baa063a4fb69df0deebdf24920ad1eb94a",
    "title": "BMCook: A Task-agnostic Compression Toolkit for Big Models",
    "abstract": "Recently, pre-trained language models (PLMs) have achieved great success on various NLP tasks and have shown a trend of exponential growth in model size. To alleviate the unaffordable computational costs brought by the size growth, model compression has been widely explored. Existing efforts have achieved promising results in compressing medium-sized models for speci\ufb01c tasks, while task-agnostic compression for big models with over billions of parameters is rarely studied. Task-agnostic compression can provide an ef\ufb01-cient and versatile big model for both prompting and delta tuning, leading to a more general impact than task-speci\ufb01c compression. Hence, we introduce a task-agnostic compression toolkit BMCook for big models. In BMCook, we implement four representative compression methods, including quantization, pruning, distillation, and MoE\ufb01cation. Devel-opers can easily combine these methods towards better ef\ufb01ciency. To evaluate BMCook, we apply it to compress T5-3B (a PLM with 3 billion parameters). We achieve nearly 12x ef-\ufb01ciency improvement while maintaining over 97% of the original T5-3B performance on three typical NLP benchmarks. Moreover, the \ufb01nal"
}