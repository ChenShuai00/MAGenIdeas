{
    "id": "8c4e2a1c421d91a75961e1a91a378cbaf4289b0f",
    "title": ": Conditional Computation of Transformer Models for Ef\ufb01cient Inference",
    "abstract": "Transformer-based pre-trained language mod-001 els achieve superior performance on most NLP 002 tasks due to large parameter capacity, but also 003 lead to huge computation cost. Fortunately, we 004 observe that most inputs only activate a tiny ra-005 tio of neurons of large Transformer-based pre-006 trained models during inference. Hence, we 007 propose to convert a model into its mixture-008 of-experts (MoE) version with the same pa-009 rameters, namely MoE \ufb01cation , which acceler-010 ates large-model inference by conditional com-011 putation based on the sparse activation phe-012 nomenon. Speci\ufb01cally, MoE\ufb01cation consists 013 of two phases: (1) splitting the parameters of 014 feed-forward neural networks (FFNs) into mul-015 tiple parts as experts, and (2) building expert 016 routers to decide which experts will be used 017 for each input. Experimental results show that 018 MoE\ufb01cation can save 80% computation cost 019 of FFNs while maintaining over 95% origi-020 nal performance for different models, includ-021 ing models with different sizes (up to 3 billion 022 parameters) and distilled models, on various 023 downstream tasks. Moreover, we \ufb01nd that the 024 MoE\ufb01ed model achieves better performance 025 than the MoE model pre-trained from scratch 026 with the same model size. We will release all 027 the code and models of this paper. 028"
}