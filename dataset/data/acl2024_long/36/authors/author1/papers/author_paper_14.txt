{
    "id": "30ef3b757cd0b9622b541f1d10a942018294ff0b",
    "title": "Mitigating Prototype Shift: Few-Shot Nested Named Entity Recognition with Prototype-Attention",
    "abstract": "Nested entities are prone to obtain similar rep-001 resentations in pre-trained language models, 002 posing challenges for Named Entity Recog-003 nition (NER), especially in the few-shot set-004 ting where prototype shifts often occur due to 005 distribution differences between the support 006 and query sets. In this paper, we regard en-007 tity representation as the combination of proto-008 type and non-prototype representations. With 009 a hypothesis that using the prototype repre-010 sentation specifically can help mitigate poten-011 tial prototype shifts, we propose a Prototype-012 Attention mechanism in the Contrastive Learn-013 ing framework (PACL) for the few-shot nested 014 NER. PACL first generates prototype-enhanced 015 span representations to mitigate the prototype 016 shift by applying a prototype attention mech-017 anism. It then adopts a novel prototype-span 018 contrastive loss to reduce prototype differences 019 further and overcome the O-type\u2019s non-unique 020 prototype limitation by comparing prototype-021 enhanced span representations with prototypes 022 and original semantic representations. Our ex-023 periments on three English, German, and Rus-024 sian nested NER datasets show that the PACL 025 outperformed seven baseline models on the 026 1-shot and 5-shot tasks in terms of F 1 score. 027 Further analyses indicate that our Prototype-028 Attention mechanism has high generality, en-029 hancing the performance of two baseline mod-030 els, and can serve as a valuable tool for NLP 031 practitioners facing few-shot nested NER tasks. 032"
}