{
    "id": "ed53449cb074fbf73b0cc233af0344c91c641ec7",
    "title": "Minimally-Supervised Relation Induction from Pre-trained Language Model",
    "abstract": "Relation Induction is a very practical task in 001 Natural Language Processing (NLP) area. In 002 practical application scenarios, people want to 003 induce more entity pairs having the same rela- 004 tion from only a few seed entity pairs. Thus, in- 005 stead of the laborious supervised setting, in this 006 paper, we focus on the minimally-supervised 007 setting where only a couple of seed entity pairs 008 per relation are provided. Although the conven- 009 tional relation induction methods have made 010 some success, their performance depends heav- 011 ily on the quality of word embeddings. The 012 great success of Pre-trained Language Mod- 013 els, such as BERT, changes the NLP area a 014 lot , and they are proven to be able to better 015 capture relation knowledge. In this paper, we 016 propose a novel method to induce relation with 017 BERT under the minimally-supervised setting. 018 Specifically, we firstly extract proper templates 019 from the corpus by using the mask-prediction 020 task in BERT to build pseudo-sentences as the 021 context of entity pairs. Then we use BERT at- 022 tention weights to better represent the pseudo- 023 sentences. In addition, We also use the Inte- 024 grated Gradient of entity pairs to iteratively 025 select better templates further. Finally, with the 026 high-quality pseudo-sentences, we can train a 027 better classifier for relation induction. Exper- 028 iments on Google Analogy Test Sets (GATS), 029 Bigger Analogy Test Set (BATS) and DiffVec 030 demonstrate that our proposed method achieves 031 state-of-the-art performance. 032"
}