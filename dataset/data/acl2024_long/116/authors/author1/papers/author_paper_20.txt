{
    "id": "b41dc85b11d49a21283df64182c5553de95f88b6",
    "title": "ZeroGen+: Self-Guided High-Quality Data Generation in Efficient Zero-Shot Learning",
    "abstract": "Nowadays, owing to the superior capacity of the large pre-trained language models (PLM), the PLM-based zero-shot learning has shown promising performances on various natural language processing tasks. There are emerging interests in further exploring the zero-shot learning potential of PLMs. Among them, Ze-roGen (Ye et al., 2022a) attempts to purely use PLM to generate data and train a tiny model without relying on any task-speci\ufb01c annotation. Despite its remarkable results, we observe that the synthesized data from PLM contains a signi\ufb01cant portion of samples with low quality, over\ufb01tting on such data greatly hampers the performance of the trained model and makes it unreliable for deployment. Since no gold data is accessible in zero-shot scenario, it is hard to perform model/data selection to prevent over\ufb01tting to the low-quality data. To address this problem, we propose a noise-robust bi-level re-weighting framework which is able to learn the per-sample weights measuring the data quality without requiring any gold data. With the learnt weights, clean subsets of different sizes can then be sampled to train the task model. We theoretically and empirically verify our method is able to construct synthetic"
}