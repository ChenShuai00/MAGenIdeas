{
    "id": "2f3c7fd7b7e27ca089611f8e2755bbbae163eb35",
    "title": "Multi-Task Pre-Training of Modular Prompt for Few-Shot Learning",
    "abstract": "Prompt tuning is a parameter-ef\ufb01cient approach to adapting pre-trained language models to downstream tasks. Although prompt tuning has been shown to match the performance of full model tuning when training data is suf-\ufb01cient, it tends to struggle in few-shot learning settings. In this paper, we present M ulti-task P re-trained M odular P rompt ( MP 2 ) to boost prompt tuning for few-shot learning. MP 2 is a set of combinable prompts pre-trained on 38 Chinese tasks. On downstream tasks, the pre-trained prompts are selectively activated and combined, leading to strong compositional generalization to unseen tasks. To bridge the gap between pre-training and \ufb01ne-tuning, we formulate upstream and downstream tasks into a uni\ufb01ed machine reading comprehension task. Extensive experiments under two learning paradigms, i.e., gradient descent and black-box tuning, show that MP 2 signi\ufb01cantly outperforms prompt tuning, full model tuning, and prior prompt pre-training methods in few-shot settings. In addition, we demonstrate that MP 2 can achieve surprisingly fast and strong adaptation to downstream tasks by merely learning 8 parameters to combine the pre-trained modular prompts."
}