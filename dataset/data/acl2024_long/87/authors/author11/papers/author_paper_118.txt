{
    "id": "8bbd0f707f39b612e0186cbff46617b0e6d65376",
    "title": "Towards Uni\ufb01ed Prompt Tuning for Few-shot Learning",
    "abstract": "Prompt-based \ufb01ne-tuning has boosted the per-001 formance of Pre-trained Language Models 002 (PLMs) on few-shot learning by employing 003 task-speci\ufb01c prompts. However, PLMs are 004 unfamiliar with the prompt-style expressions 005 during pre-training, which limits the few-shot 006 learning performance on downstream tasks. 007 It would be desirable if models can acquire 008 some prompting knowledge before task adap-009 tation. We present the Uni\ufb01ed Prompt Tun-010 ing ( UPT ) framework, leading to better few-011 shot learning for BERT-style models by ex-012 plicitly capturing prompting semantics from 013 non-target NLP datasets. In UPT , a novel 014 paradigm Prompt-Options-Verbalizer is pro-015 posed for joint prompt learning across differ-016 ent NLP tasks, forcing PLMs to capture task-017 invariant prompting knowledge. We further de-018 sign a self-supervised task named Knowledge-019 enhanced Selective Masked Language Model-020 ing to improve the PLM\u2019s generalization abil-021 ities for accurate adaptation to previously un-022 seen tasks. After multi-task learning, the PLM 023 can be \ufb01ne-tuned for any target few-shot NLP 024 tasks using the same prompting paradigm. Ex-025 periments over a variety of NLP tasks show 026 that UPT consistently outperforms state-of-027 the-arts for prompt-based \ufb01ne-tuning. 1 028"
}