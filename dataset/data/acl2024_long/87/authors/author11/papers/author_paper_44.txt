{
    "id": "f35d51b99a003053a6a8a57075f7028e00328e87",
    "title": "Reward Modeling Requires Automatic Adjustment Based on Data Quality",
    "abstract": "In Reinforcement Learning from Human Feed-001 back (RLHF), the reward model plays a cru-002 cial role in aligning language model outputs 003 with human values. The human preference data 004 used to train the reward model consists of a 005 prompt and a response pair, with humans anno-006 tating which response better aligns with human 007 value preferences. Due to the complexity and 008 subjectivity of the annotation task, multiple or-009 ganizations including OpenAI and Anthropic 010 report significant noise in the human preference 011 datasets, leading to instability and deviation in 012 reward model training from human values. We 013 discover that the difference in scores assigned 014 to response pairs by the reward model effec-015 tively indicates the quality of data, and data 016 of varying qualities show significant distinc-017 tions in reward model training. We introduce a 018 method that automatically adjusts reward mod-019 eling based on data quality, reducing the impact 020 of noise and making full use of dataset. Experi-021 ments on multiple human preference datasets 022 demonstrate that our method stabilizes reward 023 model training and significantly enhances the 024 alignment performance of RLHF. 025"
}