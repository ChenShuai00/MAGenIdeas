{
    "id": "dbf5d0fa709e6f767294b41e6bc2fe0e4bf01709",
    "title": "ezCoref : A Scalable Approach for Collecting Crowdsourced Annotations for Coreference Resolution",
    "abstract": "Large-scale high-quality corpora are critical 001 for advancing research in coreference resolu-002 tion. Coreference annotation is typically time-003 consuming and expensive, since researchers 004 generally hire expert annotators and train them 005 with an extensive set of guidelines. Crowd-006 sourcing is a promising alternative, but coref-007 erence includes complex semantic phenomena 008 difficult to explain to untrained crowdworkers, 009 and the clustering structure is difficult to ma-010 nipulate in a user interface. To address these 011 challenges, we develop and release ezCoref , 012 an easy-to-use coreference annotation tool and 013 annotation methodology that facilitates crowd-014 sourced data collection across multiple do-015 mains, currently in English. Instead of teaching 016 crowdworkers how to handle non-trivial cases 017 (e.g., near-identity coreferences), ezCoref 018 provides only a minimal set of guidelines suf-019 ficient for understanding the basics of the task. 020 To validate this decision, we deploy ezCoref 021 on Mechanical Turk to re-annotate 240 pas-022 sages from seven existing English coreference 023 datasets across seven domains, achieving an 024 average rate of 2530 tokens per hour, for one 025 annotator. This paper is the first to compare 026 the quality of crowdsourced coreference anno-027 tations against those of experts, and to identify 028 where their behavior differs to facilitate future 029 annotation efforts. We show that it is possi-030 ble to collect coreference annotations of a rea-031 sonable quality in a fraction of time it would 032 traditionally require. 033"
}