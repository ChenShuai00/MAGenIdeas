{
    "id": "fd348273d506d4119b7f2519d23827fb2a7349c8",
    "title": "Last to Learn Bias: Analyzing and Mitigating a Shortcut in Question Matching",
    "abstract": "Recent studies report that even if deep neural 001 models make correct predictions, models may 002 be relying on shortcut rather than understand-003 ing the semantics of the text. Previous stud-004 ies indicate that the shortcut deriving from the 005 biased data distribution in training set makes 006 spurious correlations between features and la-007 bels. In this paper, we focus on analyzing 008 and mitigating the biased data distribution in 009 question matching by exploring the model be-010 havior and performance. In particular, we de-011 \ufb01ne bias-word as the shortcut, and explore 012 the following questions: (1) Will the bias af-013 fect the model? (2) How does the bias affect 014 the model\u2019s decision? Our analysis reveals 015 that bias-words make signi\ufb01cantly higher con-016 tributions to model predictions than random 017 words, and the models tend to assign labels 018 that are highly correlated to the bias-words. To 019 mitigate the effects of shortcut, we propose 020 a simple approach that learns more no-bias-021 examples \ufb01rst and more bias-examples last. 022 The experiments demonstrate the effectiveness 023 of the proposed approach. 024"
}