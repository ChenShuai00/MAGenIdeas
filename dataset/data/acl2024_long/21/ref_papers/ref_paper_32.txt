{
    "id": "d51ebec3064f82ea4128fc1c3241003d4072c639",
    "title": "Truthful AI: Developing and governing AI that does not lie",
    "abstract": "In many contexts, lying \u2013 the use of verbal falsehoods to deceive \u2013 is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI \u201clies\u201d (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: 1. identify clear truthfulness standards; 2. create institutions that can judge adherence to those standards; and 3. develop AI systems that are robustly truthful. Our initial proposals for these areas include: 1. a standard of avoiding \u201cnegligent falsehoods\u201d (a generalisation of lies that is easier to assess); 2. institutions to evaluate AI systems before and after real-world deployment; 3. explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set. ar X iv :2 11 0. 06 67 4v 1 [ cs .C Y ] 1 3 O ct 2 02 1"
}