{
    "id": "6626dadc76d1af9d19fc4c2a4fa3a4cf414e62e0",
    "title": "Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks",
    "abstract": "Warning: this paper contains content that may be offensive or upsetting. The research of open-domain dialog systems has been greatly prospered by neural models trained on large-scale corpora, however, such corpora often introduce various safety problems (e.g., offensive languages, biases, and toxic behaviors) that signi\ufb01cantly hinder the deployment of dialog systems in practice. Among all these unsafe issues, addressing social bias is more complex as its negative impact on marginalized populations is usually expressed implicitly, thus requiring normative reasoning and rigorous analysis. In this paper, we focus our investigation on social bias detection of dialog safety problems. We \ufb01rst propose a novel D IAL - B IAS F RAME for analyzing the social bias in conversations pragmatically, which considers more comprehensive bias-related analyses rather than simple dichotomy annotations. Based on the proposed framework, we further introduce CD AIL -B IAS D ATASET that, to our knowledge, is the \ufb01rst well-annotated Chinese social bias dialog dataset. In addition, we establish several dialog bias detection benchmarks at different label granularities and input types (utterance-level and context-level). We show that the proposed in-depth analyses together with these benchmarks in our D IAL - B IAS F RAME are necessary and essential to bias detection tasks and can bene\ufb01t building safe dialog systems in practice."
}