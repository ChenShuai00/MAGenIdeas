{
    "id": "79680e20f0c8038039b243fb5fd385fbe967c799",
    "title": "Controlling Large Language Models to Generate Secure and Vulnerable Code",
    "abstract": "\u2014Large language models (LMs) are increasingly pre-trained on massive corpora of open-source programs and applied to solve program synthesis tasks. However, a fundamental limitation of LMs is their unawareness of security and vulnerability during pretraining and inference. As a result, LMs produce secure or vulnerable programs with high uncertainty (e.g., around 60%/40% chances for GitHub Copilot according to a recent study). This greatly impairs LMs\u2019 usability, especially in security-sensitive scenarios. To address this limitation, this work formulates a new problem called controlled code generation, which allows users to input a boolean property into an LM to control if the LM generates secure or vulnerable code. We propose svGen, an effective and lightweight learning approach for solving controlled code generation. svGen leverages property-speci\ufb01c continuous vectors to steer program generation toward the given property, without altering the weights of the LM. svGen\u2019s training optimizes those continuous vectors by carefully applying specialized loss terms on different regions of code. Our extensive evaluation shows that svGen achieves strong control capability across various software vulnerabilities and LMs of different parameter sizes. For example, on 9 dangerous vulnerabilities, a state-of-the-art CodeGen LM with 2.7B parameters generates secure programs with a 57% chance. When we use svGen to control the LM to generate secure (resp., vulnerable) programs, the chance is signi\ufb01cantly increased to 82% (resp., decreased to 35%)."
}