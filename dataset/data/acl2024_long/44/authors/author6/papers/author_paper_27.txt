{
    "id": "8ad3bc604adc58c828c30e55e9adef0a81bf7e81",
    "title": "Reducing Gender Bias in Abusive Language Detection",
    "abstract": "Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, \u201cYou are a good woman\u201d was considered \u201csexist\u201d when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98% and can be extended to correct model bias in other scenarios."
}