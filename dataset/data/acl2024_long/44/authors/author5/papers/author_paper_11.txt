{
    "id": "5b657b73316fb5101f77f3e3b8976250f0501b84",
    "title": "RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text Matching Models",
    "abstract": "With the extensive use of vision-language models in various downstream tasks, evaluating their robustness is crucial. In this paper, we propose a benchmark for assessing the robustness of vision-language models. We believe that a robust model should properly understand both linguistic and visual semantics and be resilient to explicit variations. In pursuit of this goal, we create new variants of texts and images in the MS-COCO test set and re-evaluate the state-of-the-art (SOTA) models with the new data. Specifically, we alter the meaning of text by replacing a word, and generate visually altered images that maintain some visual context while introducing noticeable pixel changes through image mixing techniques.Our evaluations on the proposed benchmark reveal substantial performance degradation in many SOTA models (e.g., Image-to-Text Recall@1: 81.9\\% $\\rightarrow$ 48.4\\% in BLIP, 66.1\\% $\\rightarrow$ 37.6\\% in VSE$\\infty$), with the models often favoring the altered texts/images over the original ones. This indicates the current vision-language models struggle with subtle changes and often fail to understand the overall context of texts and images. Based on these findings, we propose semantic contrastive loss and visual contrastive loss to learn more robust embedding. Datasets and code are available at {\\url{https://github.com/pseulki/rococo}}."
}