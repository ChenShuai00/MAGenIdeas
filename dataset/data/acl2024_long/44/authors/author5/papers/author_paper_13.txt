{
    "id": "749136bdd1892b85df0b06ce8e87198393f7e756",
    "title": "RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models",
    "abstract": "Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and cap-tions to a retrieval pool. Speci\ufb01cally, we change images by inserting unrelated images, and change captions by sub-stituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% \u2192 64.5% in BLIP, 66.1% \u2192 37.5% in VSE \u221e ). We expect that our \ufb01ndings can provide insights for improving the robustness of the vision-language models and de-vising more diverse stress-test methods in cross-modal retrieval task. Source code and dataset will be available at https://github.com/pseulki/rococo ."
}