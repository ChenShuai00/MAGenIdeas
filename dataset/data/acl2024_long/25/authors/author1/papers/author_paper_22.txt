{
    "id": "75d0c11d96dbfa60c131af66b5d97b03c0824dc2",
    "title": "Hyperlink-induced Pre-training for Passage Retrieval of Open-domain Question Answering",
    "abstract": "To alleviate the data scarcity problem in train-001 ing question answering systems, recent works 002 propose additional intermediate pre-training 003 for dense passage retrieval (DPR). However, 004 there still remains a large discrepancy be-005 tween the provided upstream signals and 006 the downstream question-passage relevance, 007 which leads to less improvement. To bridge 008 this gap, we propose the H yper L ink-induced 009 P re-training (HLP), a method to pre-train the 010 dense retriever with the text relevance induced 011 by hyperlink-based topology within Web doc-012 uments. We demonstrate that the hyperlink-013 based structures of dual-link and co-mention 014 can provide effective relevance signals for 015 large-scale pre-training that better facilitate 016 downstream passage retrieval. We investigate 017 the effectiveness of our approach across a wide 018 range of open-domain QA datasets under zero-019 shot, few-shot, multi-hop, and out-of-domain 020 scenarios. The experiments show our HLP out-021 performs the BM25 by up to 7 points as well as 022 other pre-training methods by up to 30 points 023 in terms of top-20 retrieval accuracy under the 024 zero-shot scenario. Furthermore, HLP signi\ufb01-025 cantly outperforms other pre-training methods 026 under the other scenarios. 027"
}