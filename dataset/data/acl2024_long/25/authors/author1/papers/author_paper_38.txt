{
    "id": "428bcf0dd0877eaecd60ebad49bb7c8bad5fa75f",
    "title": "SMELLM: A Paradigm to Integrating Domain Knowledge into LLMs via the Retrieval Augmented Generation",
    "abstract": "The utilization of large language models 001 (LLMs) offers promising opportunities to expe-002 dite scientific discovery. However, deploying 003 LLMs to answer scientific questions within spe-004 cific interdisciplinary research domains, such 005 as single-molecule electronics, poses various 006 challenges that arise from the uniqueness of 007 domain-specific data, the complexity of domain 008 knowledge, and the uniqueness of domain ob-009 jectives. To address this gap, we propose a 010 paradigm for integrating domain knowledge 011 from single-molecule electronics into LLMs us-012 ing the retrieval-augmented generation (RAG) 013 framework, named SMELLM. Evaluation re-014 sults demonstrate that SMELLM achieves a 015 higher SciBERT score than GPT and ChatGPT, 016 with SMELLM-4.0 notably achieving a SciB-017 ERT score of 0.731 and a Faithfulness score of 018 0.916. The responses generated by SMELLM 019 are firmly grounded in domain-specific facts, 020 indicating significant enhancements in LLM ca-021 pabilities for domain-specific natural language 022 understanding tasks. Furthermore, SMELLM 023 is adaptable for enhancing and evaluating profi-024 ciency in LLM across other scientific domains 025 with low computing resource consumption. 026"
}