{
    "id": "520711a1e93e6c4221f2a7c97c27a508379e8e37",
    "title": "Flan-MoE: Scaling Instruction-Finetuned Language Models with Sparse Mixture of Experts",
    "abstract": "The explosive growth of language models and their applications have led to an increased demand for efficient and scalable methods. In this paper, we introduce F LAN -M O E, a set of Instruction-Finetuned Sparse Mixture-of-Expert (MoE) models. We show that naively finetuning MoE models on a task-specific dataset (in other words, no instruction-finetuning) often yield worse performance compared to dense models of the same computational complexity. However, our F LAN -M O E out-performs dense models under multiple experiment settings: instruction-finetuning only and instruction-finetuning followed by task-specific finetuning. This shows that instruction-finetuning is an essential stage for MoE models. Specifically, our largest model, F LAN -M O E 32 B , surpasses the performance of F LAN -P A LM 62 B on four benchmarks, while utilizing only one-third of the FLOPs. The success of F LAN -M O E encourages rethinking the design of large-scale, high-performance language models, under the setting of task-agnostic learning."
}