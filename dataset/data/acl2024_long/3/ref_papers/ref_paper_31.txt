{
    "id": "51cda783aa6a97e0b3b5915a2bb5a35f31f3c083",
    "title": "GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models",
    "abstract": "Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch be-tween output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher\u2019s distribution. To address these issues, we propose Generalized Knowledge Distil-lation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher\u2019s distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarization, machine translation, and arithmetic reasoning tasks"
}