{
    "id": "a6db1158175f7c4c8fd6797b196ccf6584d539b0",
    "title": "Incremental Models of Natural Language Category Acquisition",
    "abstract": "Incremental Models of Natural Language Category Acquisition Trevor Fountain (t.fountain@sms.ed.ac.uk) Mirella Lapata (mlap@inf.ed.ac.uk) Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB, UK Abstract In this work, we concentrate on the task of acquiring nat- ural language (semantic) categories and examine how the statistics of the linguistic environment as approximated by large corpora influences category learning. Evidently, cate- gories are learned not only from exposure to the linguistic environment but also from our interaction with the physical world. Perhaps unsurprisingly, words that refer to concrete entities and actions are among the first words being learned as these are directly observable in the environment (Bornstein et al. 2004). Experimental evidence also shows that children respond to categories on the basis of visual features, e.g., they generalize object names to new objects often on the basis of similarity in shape and texture (Landau et al. 1998, Jones et al. 1991). Nevertheless, we focus on the acquisition of se- mantic categories from large text corpora based on the hy- pothesis that simple co-occurrence statistics can be used to capture word meaning quantitatively. The corpus-based ap- proach is attractive for modeling the development of linguis- tic categories. If simple distributional information really does form the basis of a word\u2019s cognitive representation, this im- plies that learners are sensitive to the structure of the envi- ronment during language development. As experience with a word accumulates, more information about its contexts of use becomes encoded, with a corresponding increase in the abil- ity of the language learner to use the word appropriately and make inferences about novel words of the same category. The process of learning semantic categories is necessar- ily incremental. Human language acquisition is bounded by memory and procecessing limitations, and it is implausible that children process large amounts of linguistic input at once and induce an optimal set of categories. An incremental model learns as it is applied, meaning it does not require sep- arate training and testing phases. Behavioral evidence (Born- stein and Mash 2010) suggests that this scenario more closely mirrors the process by which infants acquire categories. Hav- ing this in mind, we formulate two incremental categorization models, each differing in the way they represent categories. Both models follow the exemplar tradition \u2014 categories are denoted by a list of stored exemplars and inclusion of an un- known item in a category is determined by some notion of similarity between the item and the category exemplars. Pre- vious work (Voorspoels et al. 2008, Storms et al. 2000, Foun- tain and Lapata 2010) indicates that exemplar models perform consistently better across a broad range of natural language Learning categories from examples is a fundamental problem faced by the human cognitive system, and a long-standing topic of investigation in psychology. In this work we focus on the acquisition of natural language categories and exam- ine how the statistics of the linguistic environment influence category formation. We present two incremental models of category acquisition \u2014 one probabilistic, one graph-based \u2014 which encode different assumptions about how concepts are represented (i.e., as a set of topics or nodes in a graph). Eval- uation against gold-standard clusters and human performance in a category acquisition task suggests that the graph-based ap- proach is better suited at modeling the acquisition of natural language categories. Introduction The task of categorization, in which people cluster stimuli into categories and then use those categories to make in- ferences about novel stimuli, has long been a core problem within cognitive science. Understanding the mechanisms in- volved in categorization is essential, as the ability to gener- alize from experience underlies a variety of common mental tasks, including perception, learning, and the use of language. As a result, category learning has been one of the most ex- tensively studied aspects in human cognition, with compu- tational models that range from strict prototypes (categories are represented by a single idealized member which embod- ies their core properties; e.g., Reed 1972) to full exemplar models (categories are represented by a list of previously en- countered members; e.g., Nosofsky 1988) or combinations of the two (e.g., Griffiths et al. 2007a). Historically, the stimuli involved in such studies tend to be either concrete objects with an unbounded number of features (e.g., physical objects; Bornstein and Mash 2010) or highly abstract, with a small number of manually specified features (e.g, binary strings, colored shapes; Medin and Schaffer 1978, Kruschke 1993). Furthermore, most existing models focus on adult categorization, i.e., it is assumed that a large number of categories have already been learned. A notable exception is Anderson\u2019s (1991) rational model of categorization (see also Griffiths et al. 2007a) where it is assumed that the learner starts without any predefined categories and stimuli are clus- tered into groups as they come along. When a new stimulus is observed, it can either be assigned to one of the pre-existing clusters, or to a new cluster of its own."
}