{
    "id": "f8cc43c8890976fe9d214496a81a6a5ee36c1eec",
    "title": "Table of Contents Distributional Semantics from Text and Images Reranking Bilingually Extracted Paraphrases Using Monolingual Distributional Similarity Encoding Syntactic Dependencies by Vector Permutation Workshop Program Attribute-related Meaning Representations for Adjective- Noun Phrases in a Si",
    "abstract": "ii Introduction GEMS 2011 \u2014 GEometrical Models of Natural Language Semantics \u2014 is the third instalment in a successful series of workshops on distributional models of meaning. Since their earliest application in information retrieval, these models have become omnipresent in contemporary computational linguistics and neighboring fields. Different types of distributional models have been introduced \u2014 from the relatively simple bag-of-word, document-based and syntax-based techniques to the statistically more advanced topic models. In the field of lexical semantics, their direct applications include the construction of lexical taxonomies, the recognition of textual entailment, word sense discrimination and disambiguation, cognitive modeling, etc. Moreover, other areas of NLP, like parsing and Machine Translation, have found they can indirectly benefit from the ability of distributional models to generalize from a limited training set to unseen, but semantically similar, words. The growth of distributional semantics, however, is not without its problems. The aim of GEMS is to address two orthogonal types of current challenges. First, there is the fragmentation with regard to data sets, methods and evaluation metrics, which makes it difficult to compare studies and achieve scientific progress. We addressed this problem by providing authors with two datasets suitable for the evaluation of distributional models, together with the corpora that can be used for their construction. As a result, the performance of very different approaches can be easily compared across papers. Second, these datasets were chosen so as to reflect two of the most pressing issues in the development of distributional models nowadays: differentiation between semantic relations and compositionality. The first set, presented by Baroni and Lenci, includes concrete nouns from different semantic classes (living, non-living, etc.) with associated words for specific semantic relations such as \" attribute \" , \" category coordinate \" , \" event \" , or \" metonym \". Panchenko uses this data to compare 21 measures of semantic similarity and relatedness, based on information from WordNet, a traditional corpus, and the web. Baroni, Bruni and Binh Tran explore images as a fourth type of information. Both papers discover fundamental differences in the semantic information that is captured by these different sources of information. This paves the way for a combined, more comprehensive model. The second dataset, borrowed from Mitchell and Lapata, contains phrase similarity judgments. It makes it possible to address the evaluation of distributional models in compositional tasks. Grefenstette and Sadrzadeh show how a transitive verb can be modeled \u2026"
}