{
    "id": "d0ac0cbb6418815fc418990f5d2cd65aba4b2de1",
    "title": "Generating Diverse and High-Quality Abstractive Summaries with Variational Transformers",
    "abstract": "Existing works on abstractive summarization 001 mainly focus on boosting summarization\u2019s 002 quality (informativeness, contextual similar-003 ity). To generate summaries of both 004 high diversity and quality, we proposes the 005 Transformer+CVAE model, which integrates 006 the CVAE framework into the Transformer 007 by introducing the prior/recognition networks 008 that bridges the Transformer encoder and 009 decoder. We utilize the latent variables 010 generated in the global receptive \ufb01eld of the 011 transformer by fusing them to the starting-of-012 sequence ([SOS]) of the decoder inputs. To 013 better tune the weights of the latent variables 014 in the sequence, we designed a gated unit 015 to blend the latent representation and the 016 [SOS] token. Evaluated on the Gigaword 017 dataset, our model outperforms the state-018 of-the-art seq-to-seq models and the base 019 Transformer in diversity and quality metrics. 020 After scrutinizing the pre-training and the 021 gating mechanism we apply, we discover that 022 both schemes help improve the quality of 023 generated summaries in the CVAE framework."
}