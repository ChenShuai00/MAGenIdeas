{
    "id": "c63d73d13332e9870b51537c31423c4b1e1eb394",
    "title": "Model Sparsification Can Simplify Machine Unlearning",
    "abstract": "\u2014Recent data regulations necessitate machine unlearn- ing (MU): The removal of the effect of speci\ufb01c examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but ef\ufb01cient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsi\ufb01- cation via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearn- ing performance of an approximate unlearner, closing the approximation gap, while continuing to be ef\ufb01cient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed \u2018prune \ufb01rst, then unlearn\u2019 and \u2018sparsity-aware unlearn- ing\u2019. Extensive experiments show that our \ufb01ndings and proposals consistently bene\ufb01t MU in various scenarios including class-wise data scrubbing, random data scrubbing, and backdoor data forgetting. One highlight is the 77% unlearning ef\ufb01cacy gain of \ufb01ne-tuning (one of the simplest approximate unlearning methods) in the proposed sparsity-aware unlearning paradigm. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse."
}