{
    "id": "3b70a252b21aa28d665d935dd3652302bf5bff61",
    "title": "Multi-Sense: Commonsense Enrichment through Multi-Task Learning Anonymous ACL submission",
    "abstract": "A recent discovery showed that large pre-001 trained language models (LM) are capable of 002 encoding knowledge into their parameters, as 003 a result, serving as powerful zero-shot/few-004 shot learners of knowledge-dependent tasks. 005 However, the commonsense knowledge of such 006 models is relatively under-explored despite 007 their importance on various natural language 008 tasks. In this paper, we propose Multi-Sense 009 model that is enriched with commonsense by 010 multi-task learning (MTL) on commonsense 011 question-answering and natural language in-012 ference (NLI) tasks. We hypothesize that su-013 pervision from tasks that require commonsense 014 reasoning ability will implicitly help strengthen 015 the commonsense representation within the 016 model parameters. Empirical results demon-017 strate that the multi-task commonsense enrich-018 ment step is helpful in downstream tasks (i.e., 019 reading comprehension, fact-checking). In ad-020 dition, we demonstrate the strength of Multi-021 Sense in low resource settings by conducting a 022 few-shot learning analysis. 023"
}