{
    "id": "d7bcd6475fdff3cb3f850bda2ae01f29fdc21ee6",
    "title": "Geometric Universality of Adversarial Examples in Deep Learning",
    "abstract": "We consider the problem of adversarial examples in deep learning and attempt to provide geometric insights on their universality. Speci\ufb01cally, we de\ufb01ne adversarial directions and prove relevant results towards universality of adversarial examples with few theoretical assumptions. Our results raise attention to fully-connected layers as the last layer of most neural networks, which may be prone to adversarial examples, demanding further research in this regard. A longer version with full proofs and discussions is provided with the submission email and also here. Consider the softmax regression layer at the end of many popular neural networks for visual classi\ufb01cation tasks (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2016; He et al., 2016) and the hidden space of the input neurons to the softmax layer. Denote the hidden space of the input to softmax layer H \u2286 R l , and let h \u2208 H be this input vector, and l is the number of neurons in the \ufb01-nal hidden layer. We further denote m the number of classes. We de\ufb01ne softmax function S ( z ) : R m (cid:55)\u2192 R m as S ( z ) where z is the logits. Then the overall softmax layer could be denoted S ( W T h + b ) . The neural network classi\ufb01er \ufb01rst maps input images x to the hidden representation h with the complex multi-layer non-linear function g : X (cid:55)\u2192 H , h = g ( x ) , and then perform softmax regression to obtain a predicted label y = arg max i \u2208 [ m ] S ( W T h + b ) i . We only show results with the case H = R l here. De\ufb01nition"
}