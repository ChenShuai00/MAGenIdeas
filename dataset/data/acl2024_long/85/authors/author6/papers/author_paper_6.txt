{
    "id": "5e5c3dbd38b77b2b9334f09f2ca9c5eac3fd0433",
    "title": "Branches Switching Based Pre-Training Strategy for Version Iteration of Large Language Models",
    "abstract": "Due to the continuous emergence of online 001 data, version iteration has become an indispens-002 able requirement for Large Language Models 003 (LLMs), which exacerbates the training cost of 004 LLMs. Hence, one of the pivotal challenges for 005 LLMs is how to reduce the total training cost 006 across different versions. To achieve a better 007 balance between the pre-training performance 008 and training cost, we conduct a systematic in-009 vestigation into the impact of various learning 010 rate schedules. Extensive experiments on com-011 monly used learning rate schedules show that 012 these approaches primarily focus on the perfor-013 mance of LLMs of the current version, but over-014 look the mutual influence of training processes 015 of LLMs across different versions. To address 016 above issue, we design a pre-training strategy 017 called Branches Switching based Pre-Training 018 for the training of LLMs across different ver-019 sions. Compared with pre-training LLMs of 020 different versions from scratch, our strategy 021 reduces the total training cost to 58% while 022 maintaining optimal pre-training performance. 023"
}