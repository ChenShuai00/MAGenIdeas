{
    "id": "49702dde6ee555d3e706739ef934ca819623cc6a",
    "title": "Tackling Multi-Answer Open-Domain Questions via a Recall-then-Verify Framework",
    "abstract": "Open domain questions are likely to be open-ended and ambiguous, leading to multiple valid answers. Existing approaches typically adopt the rerank-then-read framework, where a reader reads top-ranking evidence to predict answers. According to our empirical analyses, this framework is faced with three problems: to leverage the power of a large reader, the reranker is forced to select only a few relevant passages that cover diverse answers, which is non-trivial due to unknown effect on the reader\u2019s performance; the small reading bud-get also prevents the reader from making use of valuable retrieved evidence \ufb01ltered out by the reranker; besides, as the reader generates predictions all at once based on all selected evidence, it may learn pathological dependencies among answers, i.e., whether to predict an answer may also depend on evidence of the other answers. To avoid these problems, we propose to tackle multi-answer open-domain questions with a recall-then-verify framework, which separates the reasoning process of each answer so that we can make better use of retrieved evidence while also leveraging the power of large models under the same memory constraint. Our framework achieves new state-of-the-art results on two multi-answer datasets, and predicts signi\ufb01cantly more gold answers than a rerank-then-read system with an oracle reranker."
}