{
    "id": "c79c9a89eb682f90f41ebe6729288342affcc6c4",
    "title": "RID: A Unified Framework for Conversational Recommender Systems with Pretrained Language Models",
    "abstract": "Conversational Recommender Systems (CRS), 001 which aim to recommend high-quality items to 002 users through interactive conversations, have 003 gained great research attention recently. A CRS 004 is usually composed of a recommendation mod-005 ule and a generation module. In the previous 006 work, these two modules are loosely connected 007 in the model training and are shallowly inte-008 grated during inference, where a simple switch-009 ing network or copy mechanism is adopted 010 to incorporate recommended items into gen-011 erated responses. Moreover, the current end-012 to-end neural models trained on small crowd-013 sourcing datasets (e.g., 10K dialogs in the Re-014 Dial dataset) tend to be overfitting and have 015 poor chit-chat ability. In this work, we pro-016 pose a novel unified framework called RID that 017 integrates r ecommendation i nto the d ialog gen-018 eration by introducing a vocabulary pointer. To 019 tackle the low-resource issue in CRS, we fine-020 tune the large-scale pretrained language model 021 to generate fluent and diverse responses, and 022 introduce a knowledge-aware bias learned from 023 an entity-oriented knowledge graph to enhance 024 the recommendation performance. Further-025 more, we propose to evaluate the CRS mod-026 els in an end-to-end manner, which can reflect 027 the overall performance of the entire system 028 rather than the performance of individual mod-029 ules, compared to the separate evaluations of 030 the two modules used in previous work. Exper-031 iments on the benchmark dataset ReDial show 032 our RecInDial model significantly surpasses 033 the state-of-the-art methods. More extensive 034 analyses show the effectiveness of our model. 035"
}