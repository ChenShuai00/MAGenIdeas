{
    "id": "590b7a6da46d28fdd2b268d6cefcb09e7a70de5d",
    "title": "TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat",
    "abstract": "We present a novel multi-modal chitchat dialogue dataset called TikTalk, which aims at facilitating the research of intelligent chatbots who have multi-modal sensors. It consists of 38K videos and 367K corresponding di-alogues that users generate on video social applications. In contrast to existing multi-modal dialogue datasets, we construct dialogue corpora based on video comment-reply pairs, which is more similar to chitchat in real-world dialogue scenarios and easy to scale up. Compared with previous image-based dialogue datasets, the richer types of context, including text, vision, and audio, in TikTalk lead to more diverse conversations. Data analysis shows that responses in TikTalk are in correlation with various contexts and external knowledge. It poses a great challenge for the deep understanding of multi-modal information and response generation. We evaluate several baselines in terms of different types of automatic metrics and conduct case studies. Experimental results demonstrate that integrating vision and knowledge in response generation performs the best, although there is still a large room for future improvement. Our dataset is available at https://github.com/ RUC-AIMind/TikTalk ."
}