{
    "id": "cb7fa7ee3df826628c113ba0c6db1205751d89a3",
    "title": "HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
    "abstract": "Large language models (LLMs), such as Chat-GPT, are prone to generate hallucinations, i.e., content that con\ufb02icts with the source or cannot be veri\ufb01ed by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the H allucination E valuation for Large L anguage M odels ( HELMA ) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing and alleviating hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-\ufb01ltering . Speci\ufb01cally, we \ufb01rst adopt two different sampling methods to generate hallucinated samples based on instructions, and then use an example-enhanced \ufb01ltering method to select the best one. Furthermore, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT has some probabilities to generate hallucinations and existing LLMs face great challenges in recognizing the hallucinations in text. In addition, the performance can be improved by providing external knowledge or adding reasoning steps. Our benchmark can be accessed at https://github.com/ RUCAIBox/HELMA ."
}