{
    "id": "c1372b08e382030e905d1c8751a7794ee91e9d31",
    "title": "CLIP model is an Efficient Continual Learner",
    "abstract": "The continual learning setting aims to learn new tasks over time without forgetting the previous ones. The literature reports several signi\ufb01cant efforts to tackle this problem with limited or no access to previous task data. Among such efforts, typical solutions offer sophisticated techniques involving memory replay, knowledge distillation, model regularization, and dynamic network expansion. The resulting methods have a retraining cost at each learning task, dedicated memory requirements, and setting-speci\ufb01c design choices. In this work, we show that a frozen CLIP (Contrastive Language-Image Pretraining) model offers as-tounding continual learning performance without any \ufb01ne-tuning (zero-shot eval-uation). We evaluate CLIP under a variety of settings including class-incremental, domain-incremental and task-agnostic incremental learning on \ufb01ve popular benchmarks (ImageNet-100 & 1K, CORe50, CIFAR-100, and TinyImageNet). Without any bells and whistles, the CLIP model outperforms the state-of-the-art continual learning approaches in majority of the settings. We show the effect on CLIP model\u2019s performance by varying text inputs with simple prompt templates. To the best of our knowledge, this is the \ufb01rst work to report the CLIP zero-shot performance in a continual setting. We advocate the use of this strong yet embarrass-ingly simple baseline for future comparisons in the continual learning tasks. Code is available at https://github.com/vgthengane/Continual-CLIP ."
}