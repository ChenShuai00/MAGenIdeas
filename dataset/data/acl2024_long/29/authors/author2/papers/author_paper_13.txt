{
    "id": "d57caa06a42baa90eb741a9afb10fe4fff8be82a",
    "title": "SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models",
    "abstract": "Despite achieving remarkable performance on various vision-language tasks, Transformer-based pretrained vision-language models (VLMs) still suffer from efficiency issues arising from long inputs and numerous parameters, limiting their real-world applications. However, the huge computation is redundant for most samples and the degree of redundancy and the respective components vary significantly depending on tasks and input instances. In this work, we propose an adaptive acceleration method S MART T RIM for VLMs, which adjusts the inference overhead based on the complexity of instances. Specifically, S MART T RIM incorporates lightweight trimming modules into the backbone to perform task-specific pruning on redundant inputs and parameters, without the need for additional pre-training or data augmentation. Since visual and textual representations complement each other in VLMs, we propose to leverage cross-modal interaction information to provide more critical semantic guidance for identifying redundant parts. Meanwhile, we introduce a self-distillation strategy that encourages the trimmed model to be consistent with the full-capacity model, which yields further performance gains. Experimental results demonstrate that S MART T RIM significantly reduces the computation overhead ( 2 - 3 times) of various VLMs with comparable performance (only a 1 - 2% degradation) on various vision-language tasks. Compared to previous acceleration methods, S MART T RIM attains a better efficiency-performance trade-off, demonstrating great potential for application in resource-constrained scenarios."
}