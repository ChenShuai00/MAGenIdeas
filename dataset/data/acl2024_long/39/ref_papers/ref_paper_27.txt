{
    "id": "1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a",
    "title": "Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy",
    "abstract": "Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works\u2014and some recently deployed defenses\u2014focus on \u201cverbatim memo-rization\u201d, de\ufb01ned as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization def-initions are too restrictive and fail to capture more subtle forms of memorization. Speci\ufb01cally, we design and implement an ef\ufb01cient defense based on Bloom \ufb01lters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this \u201cperfect\u201d \ufb01lter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and mini-mally modi\ufb01ed \u201cstyle-transfer\u201d prompts\u2014and in some cases even the non-modi\ufb01ed original prompts\u2014to extract memorized information. For example, instructing the model to output ALL-CAPITAL texts bypasses memorization checks based on verbatim matching. We conclude by discussing potential alternative de\ufb01nitions and why de\ufb01ning memorization is a dif\ufb01cult yet crucial open question for neural language models."
}