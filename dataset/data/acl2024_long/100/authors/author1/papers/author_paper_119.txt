{
    "id": "9db4d638af1595d710796024b1682399463403a3",
    "title": "An Empirical Study of Hierarchical Dirichlet Process Priors for Grammar Induction",
    "abstract": "Recently, there has been significant progress in applying nonparametric Bayesian models to machine learning problems. This kind of models do not assume a fixed model size (the number of parameters and/or hidden variables), but instead determine the model size from the data, with a preference towards a smaller model. One type of the nonparametric Bayesian models, the hierarchical Dirichlet process (HDP) [5], defines a distribution over an interrelated group of categorical distributions. Therefore, HDP seems to be a suitable prior for the transition probabilities of a probabilistic grammar. The advantage of using HDP as the prior is that, it can be naturally incorporated into the graphical model of the grammar, so many sophisticated inference algorithms, like variational Bayesian methods and Gibbs sampling, can be applied for grammar induction. So far a number of grammar models based on the HDP prior have been proposed, for both hidden Markov models (HMM) [5] and probabilistic context-free grammars (PCFG) [6, 7]."
}