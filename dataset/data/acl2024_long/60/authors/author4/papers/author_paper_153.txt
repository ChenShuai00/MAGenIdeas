{
    "id": "76b6b0c54fdde857d2aa0af69272477daa867c59",
    "title": "Network Compression: Memory-assisted universal coding of sources with correlated parameters",
    "abstract": "The existence of significant amount of correlation in the network traffic has stimulated the development of innetwork traffic reduction techniques since end-to-end universal compression solutions would not perform well over Internet packets due to the finite-length nature of data. Recently, we proposed a memory-assisted universal compression technique that holds a significant promise for reducing the amount of traffic in the networks. The idea is based on the observation that if a finite-length sequence from a server (source) is to be compressed and transmitted over the network, the associated universal code entails a substantial overhead. On the other hand, intermediate nodes can reduce the transmission overhead by memorizing the source statistics when forwarding the sequences from the previous communications with the server. In this paper, we extend this idea to the scenario where multiple servers are present in the network by proposing distributed network compression via memory. We consider two spatially separated sources with correlated unknown source parameters. We wish to study the universal compression of a sequence of length n from one of the sources provided that the decoder has access to (i.e., memorized) a sequence of length m from the other source. In this setup, the correlation does not arise from symbol-by-symbol dependency of two outputs from the two sources (as in Slepian-Wolf setup). Instead, the two sequences are correlated because they are originated from the two sources with unknown correlated parameters. The finite-length nature of the compression problem at hand requires considering a notion of almost lossless source coding, where coding incurs an error probability pe(n) that vanishes as sequence length n grows to infinity. We obtain bounds on the redundancy of almost lossless codes when the decoder has access to a random memory of length m as a function of the sequence length n and the permissible error probability pe(n). Our results demonstrate that distributed network compression via memory has the potential to significantly improve over conventional end-to-end compression when sufficiently large memory from previous communications is available to the decoder."
}