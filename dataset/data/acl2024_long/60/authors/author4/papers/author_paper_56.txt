{
    "id": "faba141483180e53f461c6035ce95041cfed9a8f",
    "title": "Playing Games with Implicit Human Feedback",
    "abstract": "We consider the following central question in the field of Deep Reinforcement Learning (DRL): How can we use implicit human feedback to accelerate and optimize the training of a DRL algorithm? State-of-the-art methods rely on any human feedback to be provided explicitly, requiring the active participation of humans (e.g., expert labeling, demonstrations, etc.). In this work, we investigate an alternative paradigm, where non-expert humans are silently observing (and assessing) the agent interacting with the environment. The human\u2019s intrinsic reactions to the agent\u2019s behavior is sensed as implicit feedback by placing electrodes on the human scalp and monitoring what are known as event-related electric potentials. The implicit feedback is then used to augment the agent\u2019s learning in the RL tasks. We develop a system to obtain and accurately decode the implicit human feedback, specifically error-related event potentials (ErrP), for state-action pairs in an Atari-type environment. As a baseline contribution, we demonstrate the feasibility of capturing error-potentials of a human observer watching an agent learning to play several different Atari-games using an electroencephalogram (EEG) cap, and then decoding the signals appropriately and using them as an auxiliary reward function to a DRL algorithm with the intent of accelerating its learning of the game. Building atop the baseline, we then make the following novel contributions in our work: (i) We argue that the definition of ErrP is generalizable across different environments; specifically we show that ErrP of an observer can be learned for a specific game, and the definition used as-is for another game without requiring re-learning of the error-potentials. (ii) In order to improve ErrP data efficiency, we propose a new learning framework to combine recent advances in DRL into the ErrP-based feedback system, allowing humans to provide implicit feedback only prior to the start of RL agent training. (iii) Finally, we scale the implicit human feedback (via ErrP) based RL to reasonably complex environments (games) and demonstrate the significance of our approach through synthetic and real user experiments."
}