{
    "id": "1b2a0e8af5c1f18e47e71244973ce4ace4ac6034",
    "title": "Compressed Nonparametric Language Modelling",
    "abstract": "Hierarchical Pitman-Yor Process priors are compelling methods for learning language models, out-performing point-estimate based methods. However, these models remain unpopular due to computational and statistical inference issues, such as memory and time usage, as well as poor mixing of sampler. In this work we propose a novel framework which represents the HPYP model compactly using compressed suf\ufb01x trees. Then, we develop an ef\ufb01cient approximate inference scheme in this framework that has a much lower memory footprint compared to full HPYP and is fast in the inference time. The experimental results illustrate that our model can be built on signi\ufb01cantly larger datasets compared to previous HPYP models, while being several orders of magnitudes smaller, fast for training and inference, and outperforming the perplexity of the state-of-the-art Modi\ufb01ed Kneser-Ney count-based LM smoothing by up to 15% ."
}