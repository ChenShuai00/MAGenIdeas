{
    "id": "5f628955c25004b67bcd6ca52b36c77090a9fb6d",
    "title": "GauSE: Gaussian Enhanced Self-Attention for Event Extraction",
    "abstract": "Event Extraction (EE) has benefited from pre-001 trained language models (PLMs), in which 002 the self-attention mechanism could pay atten-003 tion to the global relationship between trig-004 gers/arguments and context words to enhance 005 performance. However, existing PLM-based 006 methods are not good enough at capturing local 007 trigger/argument-specific knowledge. To this 008 end, we propose a Gau ssian enhanced S elf-009 attention E vent extraction framework (GauSE), 010 which models the syntactic-related local infor-011 mation of trigger/argument as a Gaussian bias 012 for the first time, to pay more attention to the 013 syntactic scope of the local region. Further-014 more, existing methods rarely consider multi-015 ple occurrences of the same triggers/arguments 016 in EE. We explore the global interaction strate-017 gies among multiple localness of the same trig-018 gers/arguments to fuse the corresponding dis-019 tributions and capture more latent information 020 scopes. Compared to traditional GCN-based 021 models, our methods could introduce syntac-022 tic relationships without over-smoothing prob-023 lem in deep GCN layers. Experiments on EE 024 datasets demonstrate the effectiveness and gen-025 eralization of our proposed approach. 026"
}