{
    "id": "75344c9423d0abf65391d36464efb4ed177d01ce",
    "title": "Model-Agnostic Multitask Fine-tuning for Few-shot Vision-Language Transfer Learning",
    "abstract": "Despite achieving state-of-the-art zero-shot performance, existing vision-language models, e.g., CLIP, still fall short of domain-speci\ufb01c classi\ufb01cation tasks, e.g., Fungi Classi\ufb01cation. In the context of few-shot transfer learning, traditional \ufb01ne-tuning fails to prevent highly expressive model from exploiting spurious correlations in the training data. On the other hand, although model-agnostic meta-learning (MAML) presents as a nat-ural alternative for transfer learning, the expensive computation due to implicit second-order optimization limits its use in large-scale models and datasets. In this work we aim to further improve the generalization of existing vision-language models on unseen tasks via a simple yet ef\ufb01cient \ufb01ne-tuning strategy based on uniform task sampling . We term our method as Model-Agnostic Multi-task Fine-tuning (MAMF) . Compared with MAML, MAMF discards the bi-level optimization and uses only \ufb01rst-order gradients, which makes it easily scalable and computationally ef\ufb01cient. Due to the uniform task sampling procedure, MAMF consistently outperforms the classical \ufb01ne-tuning method for few-shot transfer learning on \ufb01ve benchmark datasets. Empirically, we further discover that the effectiveness of \ufb01rst-order MAML is highly dependent on the zero-shot performance of the pre-trained model, and our simple algorithm can out-perform \ufb01rst-order MAML on more challenging datasets with low zero-shot performance. Code and processed data are publicly available for research purposes at https://github.com/MikeWangWZHL/ Multitask-Finetuning CLIP."
}