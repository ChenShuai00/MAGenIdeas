{
    "id": "2d9befb50fb7f16a215b145bac9229a26058d80a",
    "title": "SparDA: Accelerating Dynamic Sparse Deep Neural Networks via Sparse-Dense Transformation",
    "abstract": "Due to its high cost-effectiveness, sparsity has become the most important approach for building ef\ufb01cient deep-learning models. However, commodity accelerators are built mainly for ef\ufb01cient dense computation, creating a huge gap for general sparse computation to leverage. Existing solutions have to use time-consuming compiling to improve the ef\ufb01ciency of sparse kernels in an ahead-of-time manner and thus are limited to static sparsity. A wide range of dynamic sparsity opportunities is missed because their sparsity patterns are only known at runtime. This limits the future of building more biological brain-like neural networks that should be dynamically and sparsely activated. In this paper, we bridge the gap between sparse computation and commodity accelerators by proposing a system, called SparDA, for ef\ufb01ciently executing deep learning models with dynamic sparsity. We identify an important property called permutation invariant that applies to most deep learning computations. The property enables SparDA (1) to extract dynamic sparsity patterns of tensors that are only known at runtime with negligible overhead; and (2) to transform the dynamic sparse computation into an equivalent dense computation which has been extremely optimized on commodity accelerators. Extensive evaluation on diverse models shows SparDA can extract and transform dynamic sparsity with negligible overhead but brings up to 9.4x speedup over state-of-art solutions."
}