{
    "id": "60c193d47f658526ab63c860791932bc1696d74b",
    "title": "RE: A Study for Restorable Embeddings",
    "abstract": "As the number of model parameters increased, 001 large language models achieved linguistic flu-002 ency and exhibited high performance in various 003 natural language tasks without gradient updates 004 because the models could retain more knowl-005 edge. However, the large model size makes dif-006 ficult to apply the model to a task requiring 007 domain knowledge not included in the training 008 corpus, due to the fact that knowledge stored 009 in model parameters is not controllable dur-010 ing generation and model parameter updates 011 are costly. To tackle the problem, we suggest 012 separating the language model and knowledge, 013 and divide the end-to-end language model into 014 three parts: 1) encoding knowledge, 2) process-015 ing the encoded knowledge, and 3) restoring 016 the processed knowledge embedding to natural 017 language. In this paper, we propose a model 018 for learning restorable embeddings as a first 019 step toward the study to separate the language 020 model and knowledge. The experimental re-021 sults shows that the proposed model can restore 022 most knowledge in 1-2 sentences by encod-023 ing knowledge in sentence-level embeddings 024 and then restoring the embeddings back to the 025 original sentence. We also verify that the em-026 beddings generated through our method signif-027 icantly improves performance in the passage 028 retrieval task. 029"
}