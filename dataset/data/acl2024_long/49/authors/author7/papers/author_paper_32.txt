{
    "id": "d3798022e9d8eb15c2b85394193a612ef0208da3",
    "title": "Robust Fine-Tuning and Evaluation of Neural Question Generation Models",
    "abstract": "Question Generation (QG) consists of gener-001 ating a question given a context, and has a 002 wide range of real-world applications, such 003 as question answering (QA), educational sys-004 tems, and information extraction. Recently, it 005 has served to improve QA systems with few 006 available human annotations, such as for unsu-007 pervised, adversarial, and domain adaptation 008 scenarios. However, QG itself is less stud-009 ied, with little justification given for choosing 010 certain model architectures or input types. In 011 this paper, we conduct a comprehensive study 012 of neural QG models to identify important as-013 pects to be considered in their implementation. 014 First, we establish a new state-of-the-art on 015 SQuAD with T5 and BART models, outper-016 forming existing parameter-efficient models, 017 such as ERNIE-GEN and UniLM, but with less 018 parameters. Second, we show that BERTScore 019 and MoverScore have higher correlations with 020 human judgements than BLEU4 and ROUGE L , 021 which are commonly used to evaluate QG mod-022 els in various criteria. Finally, through man-023 ual assessments, we find that models trained 024 without a paragraph as context can retain gram-025 maticality and understandability , but degrade 026 answerability compared to models without the 027 answer as context. This means that the global 028 context is as important as the answer for QG 029 models to anchor the question to the answer. 1 030"
}