{
    "id": "b0057f0b5b7e9b127b195190ef99437281df9ec0",
    "title": "AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys",
    "abstract": "How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs \ufb01ne-tuned by nationally representative surveys for opinion prediction \u2013 missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and \u03c1 = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, \u03c1 = 0.98). These remarkable prediction capabilities allow us to \ufb01ll in missing trends with high con\ufb01dence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zero-shot prediction task (AUC = 0.73, \u03c1 = 0.67), highlighting challenges presented by LLMs without human responses. Further, we \ufb01nd that the best models\u2019 accuracy is lower for individuals with low socioeconomic status, racial minorities, and non-partisan af\ufb01liations but higher for ideologically sorted opinions in contemporary periods. We discuss practical constraints, socio-demographic representation, and ethical concerns regarding individual autonomy and privacy when using LLMs for opinion prediction. This paper showcases a new approach for leveraging LLMs to enhance nationally representative surveys by predicting missing responses and trends."
}