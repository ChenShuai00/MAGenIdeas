{
    "id": "f930f2ab666dc272e6bb8a8f7aa83feba8228557",
    "title": "SegMix: A Simple Structure-Aware Data Augmentation Method",
    "abstract": "Many Natural Language Processing tasks in-001 volve predicting structures, such as Syntax 002 Parsing and Relation Extraction (RE). One 003 central challenge in supervised structured pre-004 diction is the lack of high-quality annotated 005 data. The recently proposed interpolation-006 based data augmentation (DA) algorithms (i.e. 007 mixup ) augment the training set via making 008 convex interpolation between training data 009 points (Zhang et al., 2018). However, current 010 algorithms (e.g. SeqMix (Zhang et al., 2020), 011 LADA (Chen et al., 2020a)) that apply mixup 012 to language structured prediction tasks are not 013 aware of the syntactic or output structures of 014 the tasks, making their performance unstable 015 and requiring additional heuristic constraints. 016 Furthermore, SeqMix-like algorithms expect a 017 linear encoding scheme of the output structure, 018 such as BIO-Scheme for Named Entity Recog-019 nition (NER), restricting its applicability. 020 To this end, we propose SegMix , a sim-021 ple framework of interpolation-based algo-022 rithms that can adapt to both the syntactic 023 and output structures, making it robust to 024 hyper-parameters and applicable to different 025 tasks. We empirically show that SegMix con-026 sistently improves performance over several 027 strong baseline models on two structured pre-028 diction tasks (NER and RE). SegMix is a 029 \ufb02exible framework that uni\ufb01es existing rule-030 based language DA methods, creating interest-031 ing mixtures of DA techniques. Furthermore, 032 the method is easy to implement and adds neg-033 ligible overhead to training and inference. 034"
}