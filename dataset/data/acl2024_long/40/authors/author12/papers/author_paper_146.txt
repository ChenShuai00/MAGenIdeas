{
    "id": "b1d5d1774e7a9358c6549c808fba7203450acb21",
    "title": "A Cueing Strategy for Prompt Tuning in Relation Extraction",
    "abstract": "Traditional relation extraction models predic-001 t con\ufb01dence scores for each relation type 002 based on a condensed sentence representation. 003 In prompt tuning, prompt templates is used 004 to tune pre-trained language models (PLMs), 005 which outputs relation types as verbalized type 006 tokens. This strategy shows great potential 007 to support relation extraction because it is ef-008 fective to take full use of rich knowledge in 009 PLMs. However, current prompt tuning mod-010 els are directly implemented on a raw input. 011 It is weak to encode contextual features and 012 semantic dependencies of a relation instance. 013 In this paper, we designed a cueing strategy 014 which implants task speci\ufb01c cues into the in-015 put. It controls the attention of prompt tun-016 ing, which enable PLMs to learn task specif-017 ic contextual features and semantic dependen-018 cies of a relation instance. We evaluated our 019 method on two public datasets. Experiments 020 show great improvement. It exceeds state-of-021 the-art performance by more than 4.8% and 022 1.4% in terms of F1-score on the SemEval cor-023 pus and the ReTACRED corpus 1 . 024"
}