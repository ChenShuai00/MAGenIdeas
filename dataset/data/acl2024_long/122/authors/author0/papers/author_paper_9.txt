{
    "id": "65fba0eb20582ff41846feaf64cda14a532803a4",
    "title": "Perturbation type categorization for multiple adversarial perturbation robustness",
    "abstract": "Recent works in adversarial robustness have proposed defenses to improve the robustness of a single model against the union of multiple perturbation types. However, these methods still suffer signi\ufb01cant trade-offs compared to the ones speci\ufb01-cally trained to be robust against a single perturbation type. In this work, we introduce the problem of categorizing adversarial examples based on their perturbation types. We \ufb01rst theoretically show on a toy task that adversarial examples of different perturbation types constitute different distributions\u2014 making it possible to distinguish them. We support these arguments with experimental validation on multiple (cid:96) p attacks and common corruptions. Instead of training a single classi\ufb01er, we propose P ROTECTOR , a two-stage pipeline that \ufb01rst cat-egorizes the perturbation type of the input, and then makes the \ufb01nal prediction using the classi\ufb01er speci\ufb01cally trained against the predicted perturbation type. We theoretically show that at test time the adversary faces a natural trade-off between fool-ing the perturbation classi\ufb01er and the succeeding classi\ufb01er optimized with perturbation-speci\ufb01c adversarial training. This makes it challenging for an adversary to plant strong attacks against the whole pipeline. Experiments on MNIST and CIFAR-10 show that P ROTECTOR outperforms prior adversarial training-based defenses by over 5% when tested against the union of (cid:96) 1 , (cid:96) 2 , (cid:96) \u221e attacks. Additionally, our method extends to a more diverse attack suite, also showing large robustness gains against multiple (cid:96) p , spatial and recolor attacks."
}