{
    "id": "55bd6e3074ca580f10cb0345ebb27d05f43e691a",
    "title": "Observable Propagation: A Data-Efficient Approach to Uncover Feature Vectors in Transformers",
    "abstract": "A key goal of current mechanistic interpretability research in NLP is to find linear features (also called \u201cfeature vectors\u201d) for transformers: directions in activation space corresponding to concepts that are used by a given model in its computation. Present state-of-the-art methods for finding linear features require large amounts of labelled data \u2013 both laborious to acquire and computationally expensive to utilize. In this work, we introduce a novel method, called \u201cobservable propagation\u201d (in short: O B P ROP ), for finding linear features used by transformer language models in computing a given task \u2013 using almost no data . Our paradigm centers on the concept of \u201cobservables\u201d, linear functionals corresponding to given tasks. We then introduce a mathematical theory for the analysis of feature vectors: we provide theoretical motivation for why LayerNorm nonlinearities do not affect the direction of feature vectors; we also introduce a similarity metric between feature vectors called the coupling coefficient which estimates the degree to which one feature\u2019s output correlates with another\u2019s. We use O B P ROP to perform extensive qualitative investigations into several tasks, including gendered occupational bias, political party prediction, and programming language detection. Our results suggest that O B P ROP surpasses traditional approaches for finding feature vectors in the low-data regime, and that O B P ROP can be used to better understand the mechanisms responsible for bias in large language models. Code for experiments can be found at github.com/jacobdunefsky/ObservablePropagation."
}