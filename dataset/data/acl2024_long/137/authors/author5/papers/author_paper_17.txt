{
    "id": "66fcb02446c87a3f66132073533c43c320522639",
    "title": "Targeted Training for Math Word Problems with Large Language Models",
    "abstract": "After recent gains achieved by large language 001 models (LLMs) on numerical reasoning tasks, 002 it has become of interest to have LLMs teach 003 small models to improve on math word prob-004 lems (MWPs). Instructing LLMs to generate 005 Chains of Thought (CoTs) to fine-tune small 006 models is an established approach. However, 007 small models are passive in this line of work, 008 and may not be able to exploit the provided 009 training data. In this paper, we propose a novel 010 targeted training strategy to match LLM\u2019s as-011 sistance with small models\u2019 capacities. The 012 small model will proactively request LLM\u2019s 013 assistance when it sifts out confusing training 014 data. Then, LLM refines such data by succes-015 sively revising reasoning steps and reducing 016 question complexity before feeding the small 017 model. Experiments show that this targeted 018 training approach remarkably improves the per-019 formance of small models on a range of MWP 020 datasets by 12\u201325 percent, making small mod-021 els even competitive with some LLMs. 022"
}