{
    "id": "4aeef159312ffd8823bda95ff2625f87b3e45ff1",
    "title": "Hierarchical Attention Decoder for Solving Math Word Problems",
    "abstract": "To answer math word problems (MWPs), mod-001 els need to formalize equations from the source 002 text of math problems. Recently, the tree-003 structured decoder has significantly improved 004 model performance on this task by generating 005 the target equation in a tree format. However, 006 current decoders usually ignore the hierarchical 007 relationships between tree nodes and their par-008 ents, which hinders further improvement. Thus, 009 we propose a structure called hierarchical atten-010 tion tree to aid the generation procedure of the 011 decoder. As our decoder follows a graph-based 012 encoder, our full model is therefore named as 013 Graph to Hierarchical Attention Tree (G2HAT). 014 We show a tree-structured decoder with hierar-015 chical accumulative multi-head attention leads 016 to significant performance improvement and 017 reaches new state-of-the-art (SOTA) on both 018 English MAWPS and Chinese Math23k MWP 019 benchmarks. For further study, we also apply 020 pre-trained language models for G2HAT, which 021 even results in new higher performance. 022"
}