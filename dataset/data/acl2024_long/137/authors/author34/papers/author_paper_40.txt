{
    "id": "6de6943ca1ec540b1db6f6d12d9b6b7a42f02eb5",
    "title": "Thesis proposal Modeling Diversity in the Machine Learning Pipeline",
    "abstract": "Randomness is a foundation on which many aspects of the machine learning pipeline are built. From training models with stochastic gradient descent to tuning hyperparameters with random search, independent random sampling is ubiquitous. While independent sampling can be fast, it can also lead to undesirable properties, such as when samples are very similar. Abstaining from the independence assumption, we propose to examine the role of diversity in these samples, especially in cases where we have limited computation, limited space, or limited data. We address three applications: tuning hyperparameters, subsampling data, and ensemble generation. Hyperparameter optimization requires training and evaluating numerous models, which can often be time consuming. Random search allows for training and evaluating these models fully in parallel; we model diversity explicitly in this regime, and find that sets of hyperparameter assignments which are more diverse lead to better optima than conventional random search, a low discrepancy sequence, and even a sequential Bayesian optimization approach. Drawing a subset of a dataset is useful in many applications, such as when the full training set is too large to fit into memory. How these subsets are drawn is important, as their distribution may differ significantly from that of the full dataset, especially in the case when some labels are much more common than others. We address this by proposing to sample smaller datasets that are diverse, both in terms of labels and features. Ensembling models is a popular technique when minimizing task-specific errors is more important than computational efficiency or interpretability. Diversity among the models in an ensemble is directly tied to the the ensemble\u2019s error, with more diversity often leading to lower error. We propose to apply our diversity promoting techniques to ensemble generation, first when selecting data to train the base models (as in bagging), and second when choosing which already-trained models will comprise an ensemble."
}