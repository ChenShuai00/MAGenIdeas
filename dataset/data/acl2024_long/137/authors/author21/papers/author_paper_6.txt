{
    "id": "471fdf22b12363f380077a5de14078f17a104d04",
    "title": "Metadata Shaping: Natural Language Annotations for the Long Tail Anonymous",
    "abstract": "Language models (LMs) struggle to capture 001 knowledge about rare entities. To better cap-002 ture entity knowledge, a common procedure 003 in prior work is to start with a base LM such 004 as BERT and to modify the LM architecture 005 or objective function to produce a knowledge-006 aware LM . Proposed knowledge-aware LMs 007 perform well compared to base LMs on entity-008 rich tasks; however deploying, understanding, 009 and maintaining many different specialized ar-010 chitectures is challenging, and they also of-011 ten introduce additional computational costs. 012 Thus we ask: to what extent we can match the 013 quality of these architectures using a base LM 014 and only changing the data? We propose meta-015 data shaping , a method which inserts readily 016 available entity metadata, such as descriptions 017 and categorical tags, into examples at train and 018 inference time based on mutual information. 019 Intuitively, if metadata corresponding to popu-020 lar entities overlap with metadata for rare enti-021 ties, the LM may be able to better reason about 022 the rare entities using patterns learned from 023 similar popular entities. On standard entity-024 rich tasks (TACRED, FewRel, OpenEntity), 025 metadata shaping exceeds the BERT-baseline 026 by an average of 4.3 F1 points and achieves 027 state-of-the-art results. We further show the 028 gains are on average 4.4x larger for the slice of 029 examples containing tail vs. popular entities. 030"
}