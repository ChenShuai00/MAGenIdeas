{
    "id": "4dda457b208e79513e0b8661886514ac2b363a12",
    "title": "Toward Preference-Aware Story Evaluation via Ranking, Rating and Reasoning",
    "abstract": "Existing automatic story evaluation methods 001 place a premium on story coherence, deviat-002 ing from human preference. We go beyond 003 such limitation by presenting a more challeng-004 ing task of preference-aware story evalua-005 tion . Given either a machine-generated or a 006 human-written story, the task requires the ma-007 chine to output a preference score that corre-008 sponds to human preference, along with spe-009 cific ratings and comments for various aspects 010 (e.g., opening, character-shaping). To support 011 the novel task, we introduce a well-annotated 012 StoR3 dataset comprising (i) 100k ranked story 013 pairs; and (ii) a set of 46k ratings and comments 014 on various aspects of the story. To move to-015 ward preference-aware evaluation, we propose 016 a model using the upvote count as the criterion. 017 The experiments show that the scores obtained 018 by our model have a high correlation to hu-019 man preference. Additionally, we discovered 020 that the combination of aspect ratings and com-021 ments improves the performance. Our dataset 022 and benchmarks are publicly available to ad-023 vance the research of story evaluation tasks. 1 024"
}