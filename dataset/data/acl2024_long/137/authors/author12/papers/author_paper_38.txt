{
    "id": "2a7023e7d1dbd6ea0d98efd09a1f18d8599fe78f",
    "title": "PRIMER: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization",
    "abstract": "Recently proposed pre-trained generation models achieve strong performance on single-document summarization benchmarks. However, most of them are pre-trained with general-purpose objectives and mainly aim to process single document inputs. In this paper, we propose PRIMER, a pre-trained model for multi-document representation with focus on summarization that reduces the need for dataset-speci\ufb01c architectures and large amounts of \ufb01ne-tuning labeled data. Specifically, we adopt the Longformer architecture with proper input transformation and global attention to \ufb01t for multi-document inputs, and we use Gap Sentence Generation objective with a new strategy to select salient sentences for the whole cluster, called Entity Pyramid, to teach the model to select and aggregate information across a cluster of related documents. With extensive experiments on 6 multi-document summarization datasets from 3 different domains on the zero-shot, few-shot and full-supervised settings, our model, PRIMER, outperforms current state-of-the-art models on most of these settings with large margins. 1"
}