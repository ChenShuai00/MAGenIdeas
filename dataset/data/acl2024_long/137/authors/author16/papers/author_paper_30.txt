{
    "id": "d7a5b73532635b6fa429c634519c70935887cf26",
    "title": "A Study of Pre-trained Language Models for Analogy Generation",
    "abstract": "We propose a novel application of Pre-trained 001 Language Models (PLMs) to generate analo-002 gies and study how to design effective prompts 003 to prompt a PLM to generate a source concept 004 analogous to a given target concept as well as 005 to generate an explanation of the similarity be-006 tween given pair of target concept and source 007 concept. We found that it is feasible to prompt 008 a GPT-3 PLM to generate meaningful analogies 009 and the best prompts tend to be precise impera-010 tive statements especially with low temperature 011 setting. We systematically analyzed the sen-012 sitivity of the GPT-3 model to prompt design 013 and temperature and found that the model is 014 particularly sensitive to certain variations (e.g., 015 questions vs. imperative statements). We also 016 investigated the suitability of using the exist-017 ing reference-based metrics designed for eval-018 uating natural language generation (NLG) to 019 evaluate analogy generation and found that the 020 recent BLEURT score is better than the oth-021 ers. We further propose a promising consensus 022 measure based on diverse prompts and settings, 023 which can be potentially used to both automati-024 cally evaluate the generated analogies in the ab-025 sence of reference text (e.g., in novel domains) 026 and rank a set of generated analogies to select 027 analogies of different characteristics. Overall, 028 our study shows that PLMs offer a promising 029 new way to generate analogies in unrestricted 030 domains, breaking the limitation of existing 031 analogy generation methods in requiring struc-032 tured representation. 033"
}