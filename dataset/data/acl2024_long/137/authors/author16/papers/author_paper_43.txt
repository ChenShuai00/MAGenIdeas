{
    "id": "de83aace8d37864b882b6faaf12a3b7a7178ea2e",
    "title": "Do We Need to Differentiate Negative Candidates Before Training a Neural Ranker?",
    "abstract": "Retrieval-based Question Answering (ReQA) 001 requires a system to \ufb01nd candidates (e.g., sen-002 tences or short passages) containing the an-003 swer to a given question from a large corpus. 004 A promising way to solve this task is a two-005 stage pipeline, where the \ufb01rst stage retrieves a 006 set of candidates, and the second stage uses a 007 neural network to rank the retrieved candidates. 008 There are three standard methods to train neu-009 ral rankers, Binary Cross Entropy loss, Mean 010 Square Error loss, and Hinge loss. While all 011 these training strategies assign the same la-012 bel for all the negative candidates, we argue 013 that negativeness is not binary but exists as a 014 spectrum, i.e., some candidates may be more 015 negative than the others, and thus should be 016 treated differently. We present SCONER\u2014 017 scoring negative candidates before training 018 neural ranker\u2014a model trained to differentiate 019 negative candidates. Our approach includes 020 1) semantic textual similarity-based scoring to-021 gether with data augmentation for score gener-022 ation of negative candidates; and 2) a neural 023 ranker trained on data using generated scores 024 as labels. Together, we systematically com-025 pare three standard training methods and our 026 proposed method on a range of ReQA datasets 027 under multiple settings (i.e., single-domain 028 and multi-domain). Our \ufb01nding suggests that 029 using more negative candidates to train neural 030 rankers are better than less in both single-and 031 multi-domain settings, where SCONER is the 032 best in the single-domain settings and Hinge 033 loss is the best in multi-domain settings."
}