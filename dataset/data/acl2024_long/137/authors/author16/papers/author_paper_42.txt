{
    "id": "dbeff5429ff0caa85f9e02621928e787e789ca2b",
    "title": "Hey AI, Can You Solve Complex Tasks by Talking to Agents?",
    "abstract": "Training giant models from scratch for each complex task is resource- and data-inefficient. To help develop models that can leverage existing systems, we propose a new challenge: Learning to solve complex tasks by communicating with existing agents (or models) in natural language. We design a synthetic benchmark, CommaQA, with three complex reasoning tasks (explicit, implicit, numeric) designed to be solved by communicating with existing QA agents. For instance, using text and table QA agents to answer questions such as \u201cWho had the longest javelin throw from USA?\u201d. We show that black-box models struggle to learn this task from scratch (accuracy under 50%) even with access to each agent\u2019s knowledge and gold facts supervision. In contrast, models that learn to communicate with agents outperform black-box models, reaching scores of 100% when given gold decomposition supervision. However, we show that the challenge of learning to solve complex tasks by communicating with existing agents without relying on any auxiliary supervision or data still remains highly elusive. We will release CommaQA, along with a compositional generalization test split, to advance research in this direction."
}