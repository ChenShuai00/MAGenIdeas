{
    "id": "66d5ea65186f21f4ff786aa6a1520243ecec94cb",
    "title": "Exploring the Challenges of Open Domain Multi-Document Summarization",
    "abstract": "Multi-document summarization (MDS) has traditionally been studied assuming a set of ground-truth topic-related input documents is provided. In practice, the input document set is unlikely to be available a priori and would need to be retrieved based on an information need, a setting we call open-domain MDS. We experiment with current state-of-the-art retrieval and summarization models on several popular MDS datasets extended to the open-domain setting. We \ufb01nd that existing summa-rizers suffer large reductions in performance when applied as-is to this more realistic task, though training summarizers with retrieved inputs can reduce their sensitivity retrieval errors. To further probe these \ufb01ndings, we conduct perturbation experiments on summarizer inputs to study the impact of different types of document retrieval errors. Based on our re-sults, we provide practical guidelines to help facilitate a shift to open-domain MDS. We release our code and experimental results along-side all data or model artifacts created during our investigation. 1"
}