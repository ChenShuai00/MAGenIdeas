{
    "id": "ae9cb0984a176dba838c340ce97daa8a9b21a1e5",
    "title": "GACT: Activation Compressed Training for General Architectures",
    "abstract": "Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT\u2019s approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1 \u00d7 , enabling training with a 4.2 \u00d7 to 24.7 \u00d7 larger batch size, with negligible accuracy loss."
}