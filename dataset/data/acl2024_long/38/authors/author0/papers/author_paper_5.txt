{
    "id": "71ab9e4886e2eb7db8d61fcd6a69a0d11a42b365",
    "title": "States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly Anonymous ACL submission",
    "abstract": "Large Language Models (LLMs) exhibit vari-001 ous emergent abilities. Among these abilities, 002 some might reveal the internal working mech-003 anisms of models. In this paper, we uncover a 004 novel emergent capability in models: the intrin-005 sic ability to perform extended sequences of cal-006 culations without relying on chain-of-thought 007 step-by-step solutions. Remarkably, the most 008 advanced models are capable of directly out-009 putting the results of two-digit number addi-010 tions with lengths extending up to 15 addends. 011 We hypothesize that the model emerges discrete 012 representations of symbols within its hidden 013 states and performs symbolic calculations inter-014 nally. To test this hypothesis, we design a se-015 quence of experiments that look into the hidden 016 states. Specifically, we first confirm that Im-017 plicit Discrete State Representations (IDSRs) 018 exist. Then, we provide interesting observa-019 tions about the formation of IDSRs from layer, 020 digit, and sequence perspectives. Finally, we 021 confirm that models indeed use IDSRs to pro-022 duce the final answers. However, we also dis-023 cover that the state representations are far from 024 lossless in current open-sourced models, lead-025 ing to inaccuracies in final performance. Our 026 work presents a novel exploration of LLMs\u2019 027 symbolic calculation abilities and the underly-028 ing mechanisms. 029"
}