{
    "id": "2db77485736cf29778a4464fe500a289bd46e7ac",
    "title": "ChatGPT as a Factual Inconsistency Evaluator for Abstractive Text Summarization",
    "abstract": "The performance of abstractive text summarization has been greatly boosted by pre-trained language models recently. The main concern of existing abstractive summarization methods is the factual inconsistency problem of their generated summary. To alleviate the problem, many efforts have focused on developing effective factuality evaluation metrics based on natural language inference and question answering et al. However, they have limitations of high computational complexity and relying on annotated data. Most recently, large language models such as ChatGPT have shown strong ability in not only natural language understanding but also natural language inference. In this paper, we study the factual inconsistency evaluation ability of Chat-GPT under the zero-shot setting by evaluating it on the coarse-grained and \ufb01ne-grained factuality evaluation tasks including binary natu-ral language inference (NLI), summary ranking, and consistency rating. Experimental re-sults show that ChatGPT outperforms previous SOTA evaluation metrics on 6/9 datasets across three tasks, demonstrating its great potential for assessing factual inconsistency in the zero-shot setting. The results also highlight the importance of prompt design and the need for future efforts to address ChatGPT\u2019s limitations on evaluation bias, wrong reasoning, and hallucination."
}