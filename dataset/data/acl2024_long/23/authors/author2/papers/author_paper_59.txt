{
    "id": "5b8c127cd01ee237c45c2841ac78cd69b48a87c3",
    "title": "HyperQA: Hyperbolic Embeddings for Fast and Efficient Ranking of Question Answer Pairs",
    "abstract": "Deep learning has become a fundamental component in question answering (QA) research. It is a well established fact that the top QA systems are mostly comprised of neural ranking architectures. Model a\u0089er model, we see architectural innovations ranging from word interaction layers to creative new methods for learning a\u008aentions. It also seems like these complex interaction mechanisms are mandatory for good performance. Unfortunately, many of these mechanisms incur a prohibitive computation and memory cost making them unfavorable for practical applications. In lieu of that, this paper tackles the question of whether it is possible to achieve competitive performance with very simple neural networks. As such, we propose a simple but novel deep learning architecture for fast and e\u0081cient QA. Speci\u0080cally, we present HyperQA, a parameter e\u0081cient neural network model that outperforms several parameter heavy models such as A\u008aentive Pooling BiLSTMs and Multi Perspective CNN on multiple standard benchmark datasets such as TrecQA, WikiQA and YahooQA. \u008ce key innovation to our model is a pairwise ranking objective that performs QA matching in Hyperbolic space instead of Euclidean space. \u008cis empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated a\u008aention mechanisms nor over-parameterized layers and yet outperforms many models that have these functionalities on multiple benchmarks."
}