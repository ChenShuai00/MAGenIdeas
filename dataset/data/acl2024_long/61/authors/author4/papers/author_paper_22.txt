{
    "id": "acf2dd4e2853f90832c01c556a2e716e7c720bc2",
    "title": "G ENIE A Leaderboard for Human-in-the-Loop Evaluation of Text Generation",
    "abstract": "Leaderboards have eased model development for many NLP datasets by standardizing their evaluation and delegating it to an independent external repository. Their adoption, however, is so far limited to tasks which can be reliably evaluated in an automatic manner. This work introduces G ENIE , an extensible human evaluation leaderboard, which brings the ease of leaderboards to text generation tasks. G E - NIE automatically posts leaderboard submissions to crowdsourcing platforms asking human annotators to evaluate them on various axes (e.g., correctness, conciseness, \ufb02uency), and compares their answers to various automatic metrics. We introduce several datasets in English to G ENIE , representing four core challenges in text generation: machine translation, summarization, commonsense reasoning, and machine comprehension. We provide formal granular evaluation metrics and identify areas for future research. We make G ENIE publicly available, 1 and hope that it will spur progress in language generation models as well as their automatic and manual evaluation."
}